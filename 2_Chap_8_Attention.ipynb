{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2 Chap 8-Attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqvKAcBXvPRXDzDkCxp17I",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superbunny38/2021DeepLearning/blob/main/2_Chap_8_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch-2.git original\n",
        "!mkdir workspace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTZ_FZwXRf0Q",
        "outputId": "dbe89190-7d16-4188-ca15-449255004ae6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'original'...\n",
            "remote: Enumerating objects: 606, done.\u001b[K\n",
            "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 606 (delta 1), reused 5 (delta 0), pack-reused 598\u001b[K\n",
            "Receiving objects: 100% (606/606), 29.82 MiB | 11.31 MiB/s, done.\n",
            "Resolving deltas: 100% (361/361), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/original/common workspace\n",
        "!mv /content/original/dataset workspace\n",
        "!mv /content/original/ch07 workspace\n",
        "!mv /content/original/ch08 workspace"
      ],
      "metadata": {
        "id": "joK6HoVrRjoB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd workspace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2W9uvuoRu5Z",
        "outputId": "dca3b006-610c-45d8-a20f-4d5e6b24353f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/workspace\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중합 구하기"
      ],
      "metadata": {
        "id": "RDA5aC5CNVdj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "T, H = 5,4#시계열 길이, 은닉 상태 벡터의 원소 수\n",
        "hs = np.random.randn(T,H)\n",
        "a = np.array([0.8,0.1,0.03,0.05,0.02])\n",
        "\n",
        "ar = a.reshape(5,1).repeat(4,axis=1)\n",
        "print(ar.shape)\n",
        "\n",
        "t = hs*ar\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t,axis=0)\n",
        "print(c.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN67tQLbkZEB",
        "outputId": "80bdd39f-225a-45a8-fce6-3c76d8507df1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 4)\n",
            "(5, 4)\n",
            "(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "미니배치 처리용 가중합"
      ],
      "metadata": {
        "id": "2MAv7TbEOvDt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N,T,H = 10,5,4\n",
        "hs = np.random.randn(N,T,H)\n",
        "a = np.random.randn(N,T)\n",
        "ar = a.reshape(N,T,1).repeat(H, axis = 2)\n",
        "\n",
        "t = hs*ar\n",
        "print(t.shape)\n",
        "\n",
        "c = np.sum(t, axis = 1)\n",
        "print(t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svgBMD4_khTD",
        "outputId": "19654c03-968e-4cfc-d782-c6b4f8f9346a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5, 4)\n",
            "(10, 5, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "가중합 계산 그래프"
      ],
      "metadata": {
        "id": "aTW2c_GzPMOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WeightSum:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [],[]\n",
        "    self.cache = None\n",
        "  \n",
        "  def forward(self, hs,a):\n",
        "    N, T, H = hs.shape\n",
        "    ar = a.reshape(N,T,1).repeat(H, axis = 2)\n",
        "    t = hs*ar\n",
        "\n",
        "    c = np.sum(t, axis = 1)\n",
        "\n",
        "    self.cache = (hs,ar)\n",
        "    return c\n",
        "  \n",
        "  def backward(self, dc):\n",
        "    hs, ar = self.cache\n",
        "    N,T,H = hs.shape\n",
        "\n",
        "    dt = dc.reshape(N,1,H).repeat(T, axis = 1)#sum의 역전파\n",
        "    dar = dt*hs\n",
        "    dhs = dt*ar\n",
        "    da = np.sum(dar, axis = 2)#repeat의 역전파\n",
        "\n",
        "    return dhs, da"
      ],
      "metadata": {
        "id": "hbYSzgB0kka9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder의 LSTM 계층의 은닉상태 벡터인 hs와 Decoder의 LSTM 꼐층의 은닉상태 벡터인 h의 각 단어 벡터가 얼마나 비슷한가를 내적을 통해 수치로 나타내고 softmax 함수로 정규화"
      ],
      "metadata": {
        "id": "0EKAHk8LR0YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.layers import Softmax\n",
        "import numpy as np\n",
        "\n",
        "N, T, H = 10,5,4\n",
        "hs = np.random.randn(N,T,H)\n",
        "h = np.random.randn(N,H)\n",
        "hr = h.reshape(N,1,H).repeat(T, axis = 1)\n",
        "\n",
        "t = hs*hr\n",
        "print(t.shape)\n",
        "\n",
        "s = np.sum(t,axis = 2)\n",
        "print(s.shape)\n",
        "\n",
        "softmax = Softmax()\n",
        "a = softmax.forward(s)\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycZ5NglOkoPp",
        "outputId": "348cce97-25a2-4dab-b3a3-4dc504d6c1af"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 5, 4)\n",
            "(10, 5)\n",
            "(10, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.np import *\n",
        "from common.layers import Softmax\n",
        "class AttentionWeight:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [],[]\n",
        "    self.softmax = Softmax()\n",
        "    self.cache = None\n",
        "  \n",
        "  def forward(self,hs,h):\n",
        "    N,T,H = hs.shape\n",
        "    hr = h.reshape(N,1,H).repeat(T, axis = 1)\n",
        "    t = hs*hr\n",
        "    s = np.sum(t, axis = 2)\n",
        "    a = self.softmax.forward(s)\n",
        "\n",
        "    self.cache = (hs,hr)\n",
        "    return a\n",
        "\n",
        "  def backward(self,da):\n",
        "    hs, hr = self.cache\n",
        "    N,T,H = hs.shape\n",
        "\n",
        "    ds = self.softmax.backward(da)\n",
        "    dt = ds.reshape(N,T,1).repeat(H, axis = 2)\n",
        "    dhs = dt*hr\n",
        "    dhr = dt*hs\n",
        "    dh = np.sum(dhr, axis =1)\n",
        "    return dhs, dh"
      ],
      "metadata": {
        "id": "MuDtX_gcSZxl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder가 건네주는 정보 hs에서 중요한 원소에 주목하여 그것을 바탕을 맥락 벡터를 구해 위쪽 계층으로 전파"
      ],
      "metadata": {
        "id": "z-ODKFFZTXKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [],[]\n",
        "    self.attention_weight_layer = AttentionWeight()\n",
        "    self.weight_sum_layer = WeightSum()\n",
        "    self.attention_weight = None\n",
        "  \n",
        "  def forward(self, hs, h):\n",
        "    a = self.attention_layer.forward(hs,h)\n",
        "    out = self.weight_sum_layer.forward(hs, a)\n",
        "    self.attention_weight = a\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    dhs0, da = self.weight_sum_layer.backward(dout)\n",
        "    dhs1, dh = self.attention_weight_layer.backward(da)\n",
        "    dhs = dhs0 + dhs1\n",
        "    return dhs, dh"
      ],
      "metadata": {
        "id": "AQE81MqVTM7T"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "다수의 Attention 계층을 Time Attention 계층으로 모아 구현"
      ],
      "metadata": {
        "id": "ANYpmtWNURHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeAttention:\n",
        "  def __init__(self):\n",
        "    self.params, self.grads = [],[]\n",
        "    self.layers = None\n",
        "    self.attention_weights = None\n",
        "  \n",
        "  def forward(self, hs_enc, hs_dec):\n",
        "    N,T,H = hs_dec.shape\n",
        "    out = np.empty_like(hs_dec)\n",
        "    self.layers = []\n",
        "    self.attention_weights = []\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = Attention()\n",
        "      out[:,t,:] = layer.forward(hs_enc, hs_dec[:,t,:])\n",
        "      self.layers.append(layer)\n",
        "      self.attention_weights.append(layer.attention_weight)\n",
        "    return out\n",
        "  \n",
        "  def backward(self, dout):\n",
        "    N,T,H = dout.shape\n",
        "    dhs_enc = 0\n",
        "    dhs_dec = np.empty_like(dout)\n",
        "\n",
        "    for t in range(T):\n",
        "      layer = self.layers[t]\n",
        "      dhs, dh = layer.backward(dout[:,t,:])\n",
        "      dhs_enc += dhs\n",
        "      dhs_dec[:,t,:] = dh\n",
        "    return dhs_enc, dhs_dec"
      ],
      "metadata": {
        "id": "yqPCi_DRUBZN"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention을 갖춘 seq2seq 구현"
      ],
      "metadata": {
        "id": "9qEgqyWdVE4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention"
      ],
      "metadata": {
        "id": "IfAKtE00VCNb"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionEncoder(Encoder):\n",
        "  def forward(self, xs):\n",
        "    xs = self.embed.forward(xs)\n",
        "    hs = self.lstm.forward(xs)#모든 은닉상태 반환\n",
        "    return hs\n",
        "  \n",
        "  def backward(self, dhs):\n",
        "    dout = self.lstm.backward(dhs)\n",
        "    dout = self.embed.backward(dout)\n",
        "    return dout"
      ],
      "metadata": {
        "id": "eFom_9eIVbY1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttention()\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:,-1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)#LSTM 계층의 출력\n",
        "        c = self.attention.forward(enc_hs, dec_hs)#Time Attention 계층의 출력\n",
        "        out = np.concatenate((c, dec_hs), axis=2)# 두 출력 연결\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        N, T, H2 = dout.shape\n",
        "        H = H2 // 2\n",
        "\n",
        "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
        "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "        ddec_hs = ddec_hs0 + ddec_hs1\n",
        "        dout = self.lstm.backward(ddec_hs)\n",
        "        dh = self.lstm.dh\n",
        "        denc_hs[:, -1] += dh\n",
        "        self.embed.backward(dout)\n",
        "\n",
        "        return denc_hs\n",
        "\n",
        "    def generate(self, enc_hs, start_id, sample_size):#새로운 단어열 생성\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "            out = self.embed.forward(x)\n",
        "            dec_hs = self.lstm.forward(out)\n",
        "            c = self.attention.forward(enc_hs, dec_hs)\n",
        "            out = np.concatenate((c, dec_hs), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(sample_id)\n",
        "\n",
        "        return sampled\n"
      ],
      "metadata": {
        "id": "soqLl0-KVp_z"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionSeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        args = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = AttentionEncoder(*args)\n",
        "        self.decoder = AttentionDecoder(*args)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "metadata": {
        "id": "RhVKSnWhWb_p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Seq2seq, Encoder\n",
        "\n",
        "\n",
        "class PeekyDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        N, T = xs.shape\n",
        "        N, H = h.shape\n",
        "\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        out = self.lstm.forward(out)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        score = self.affine.forward(out)\n",
        "        self.cache = H\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        H = self.cache\n",
        "\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
        "        self.embed.backward(dembed)\n",
        "\n",
        "        dhs = dhs0 + dhs1\n",
        "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        char_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        H = h.shape[1]\n",
        "        peeky_h = h.reshape(1, 1, H)\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([char_id]).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            out = self.lstm.forward(out)\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            char_id = np.argmax(score.flatten())\n",
        "            sampled.append(char_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "\n",
        "class PeekySeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = PeekyDecoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "metadata": {
        "id": "8s3tSQvYX59g"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 날짜 형식 변환 문제"
      ],
      "metadata": {
        "id": "_H5LQafbW3Ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "sys.path.append('../ch07')\n",
        "import numpy as np\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam"
      ],
      "metadata": {
        "id": "2Z_jP7OIWlNR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#학습 코드\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "\n",
        "from ch07.seq2seq import Seq2seq"
      ],
      "metadata": {
        "id": "lYWexes3XIG4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#데이터 읽기\n",
        "(x_train, t_train),(x_test, t_test) = sequence.load_data('date.txt')"
      ],
      "metadata": {
        "id": "setZUmAxXZrR"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_id, id_to_char = sequence.get_vocab()"
      ],
      "metadata": {
        "id": "ftTx_vQgYCSv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#입력 문장 반전\n",
        "x_train, x_test = x_train[:,::-1],x_test[:,::-1]"
      ],
      "metadata": {
        "id": "9BularO0YGRv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#하이퍼파라미터 설정\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0"
      ],
      "metadata": {
        "id": "IFjU6SHiYKhu"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)"
      ],
      "metadata": {
        "id": "1tv8-58kYV-v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_list = []\n",
        "for epoch in range(max_epoch):\n",
        "  trainer.fit(x_train, t_train, max_epoch = 1,\n",
        "              batch_size = batch_size, max_grad = max_grad)\n",
        "  correct_num = 0#for acc\n",
        "  for i in range(len(x_test)):\n",
        "    question, correct = x_test[[i]], t_test[[i]]#?왜지?[[]]?\n",
        "    verbose = i<10\n",
        "    correct_num += eval_seq2seq(model, question, correct, id_to_char, verbose, is_reverse = True)\n",
        "  \n",
        "  acc = float(correct_num)/len(x_test)\n",
        "  acc_list.append(acc)\n",
        "  print('val acc %.3f%%'%(acc*100))\n",
        "model.save_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cRR1m1WYcum",
        "outputId": "b6d43200-464e-47f0-91f6-701d93068e90"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 13[s] | 손실 3.09\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 25[s] | 손실 1.90\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 38[s] | 손실 1.72\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 51[s] | 손실 1.46\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 63[s] | 손실 1.19\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 75[s] | 손실 1.14\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 88[s] | 손실 1.09\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 103[s] | 손실 1.06\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 117[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 131[s] | 손실 1.03\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 145[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 161[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 183[s] | 손실 1.01\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 197[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 210[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 222[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 235[s] | 손실 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1978-08-11\n",
            "---\n",
            "val acc 0.000%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 13[s] | 손실 1.00\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 25[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 38[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 51[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 64[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 77[s] | 손실 0.99\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 90[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 103[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 115[s] | 손실 0.97\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 128[s] | 손실 0.95\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 141[s] | 손실 0.94\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 153[s] | 손실 0.90\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 166[s] | 손실 0.83\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 178[s] | 손실 0.74\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 190[s] | 손실 0.66\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 202[s] | 손실 0.58\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 215[s] | 손실 0.47\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 2006-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2007-08-09\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1983-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2016-11-08\n",
            "---\n",
            "val acc 51.320%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.35\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 12[s] | 손실 0.30\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 25[s] | 손실 0.21\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 37[s] | 손실 0.14\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 50[s] | 손실 0.09\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 62[s] | 손실 0.07\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 75[s] | 손실 0.05\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 87[s] | 손실 0.04\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 100[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 113[s] | 손실 0.03\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 126[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 139[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 152[s] | 손실 0.02\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 164[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 177[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 190[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 203[s] | 손실 0.01\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 215[s] | 손실 0.01\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 99.900%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 13[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 26[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 39[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 52[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 65[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 78[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 91[s] | 손실 0.01\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 104[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 117[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 130[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 143[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 156[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 169[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 182[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 195[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 208[s] | 손실 0.00\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 220[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 99.900%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 26[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 39[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 51[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 64[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 77[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 103[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 116[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 128[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 141[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 154[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 167[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 180[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 193[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 205[s] | 손실 0.00\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 218[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 26[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 39[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 52[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 64[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 77[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 89[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 102[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 115[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 128[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 140[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 153[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 165[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 178[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 191[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 203[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 216[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 25[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 38[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 50[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 63[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 75[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 87[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 100[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 125[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 137[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 150[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 162[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 175[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 187[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 200[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 212[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 25[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 38[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 50[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 63[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 75[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 87[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 100[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 125[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 137[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 150[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 162[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 175[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 187[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 200[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 212[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 25[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 38[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 50[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 63[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 75[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 88[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 100[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 125[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 137[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 150[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 162[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 175[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 187[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 199[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 212[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 13[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 25[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 37[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 50[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 62[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 75[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 87[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 99[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 112[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 124[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 137[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 149[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 162[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 174[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 186[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 199[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 211[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "val acc 100.000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "plot accuracy "
      ],
      "metadata": {
        "id": "zamyGMClZxPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(acc_list)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "3RiKQhoYZztl",
        "outputId": "3964ae88-cb95-4cc1-ea50-0c69ce345e63"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Attention"
      ],
      "metadata": {
        "id": "XYk3ZANbZkxB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "import numpy as np\n",
        "from dataset import sequence"
      ],
      "metadata": {
        "id": "r-n6wSpzZAKC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "(x_train, t_train), (x_test, t_test) = \\\n",
        "    sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 입력 문장 반전\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "model.load_params()\n",
        "\n",
        "_idx = 0\n",
        "def visualize(attention_map, row_labels, column_labels):\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.pcolor(attention_map, cmap=plt.cm.Greys_r, vmin=0.0, vmax=1.0)\n",
        "\n",
        "    ax.patch.set_facecolor('black')\n",
        "    ax.set_yticks(np.arange(attention_map.shape[0])+0.5, minor=False)\n",
        "    ax.set_xticks(np.arange(attention_map.shape[1])+0.5, minor=False)\n",
        "    ax.invert_yaxis()\n",
        "    ax.set_xticklabels(row_labels, minor=False)\n",
        "    ax.set_yticklabels(column_labels, minor=False)\n",
        "\n",
        "    global _idx\n",
        "    _idx += 1\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "np.random.seed(1984)\n",
        "for _ in range(5):\n",
        "    idx = [np.random.randint(0, len(x_test))]\n",
        "    x = x_test[idx]\n",
        "    t = t_test[idx]\n",
        "\n",
        "    model.forward(x, t)\n",
        "    d = model.decoder.attention.attention_weights\n",
        "    d = np.array(d)\n",
        "    attention_map = d.reshape(d.shape[0], d.shape[2])\n",
        "\n",
        "    # 출력하기 위해 반전\n",
        "    attention_map = attention_map[:,::-1]\n",
        "    x = x[:,::-1]\n",
        "\n",
        "    row_labels = [id_to_char[i] for i in x[0]]\n",
        "    column_labels = [id_to_char[i] for i in t[0]]\n",
        "    column_labels = column_labels[1:]\n",
        "\n",
        "    visualize(attention_map, row_labels, column_labels)#의미있는 것끼리 각각 대응하는걸 알 수 있다"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HCAbXpN0Zo48",
        "outputId": "8338d39a-e880-4890-fd0c-950e14d91dfd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}