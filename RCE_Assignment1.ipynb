{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RCE-Assignment1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42CRdtcCRTuc"
      },
      "source": [
        "# Assignment 1 – Polynomial Regression using ``torch.nn.Module``\n",
        "\n",
        "- Please create a copy of this notebook onto your own Drive before working on it.\n",
        "- Please submit your ipynb file named with your initials, e.g. `YGM-Assignment1.ipynb` with **the CODE cells output visible** to support your answers and **TEXTUAL answers given as comments** in the code cells.\n",
        "- Deadline for submission is **midnight, Friday, September 24th.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KvsIo2UfwO3"
      },
      "source": [
        "## Neural Network Model for Polynomial Regression\n",
        "Your task is to build a neural network for the function $y = x^2 + 3x$\n",
        "\n",
        "Requirements:\n",
        "- You MUST use `torch.nn.Module` to define your neural network class.\n",
        "- The training data should have **10 input values, $x$, and the correct corresponding output values, $y$,** for the function $y = x^2 + 3x$\n",
        "- The NN may have **maximum TWO hidden layers**.\n",
        "- You may use a **maximum of 500 neuron units in each hidden layer**.\n",
        "- You may train over a **maximum of 1000 epochs**.\n",
        "- Use suitable activation functions that have been covered in class.\n",
        "- You MUST use the **Adam optimiser, `torch.optim.Adam()`** and the **MSE loss function**.\n",
        "- **IMPORTANT:** Your model must have a **LOSS OF LESS THAN 0.01** at the end of training.\n",
        "- **Train your model at least 3 times** to see that the final loss value is stable across all three runs.\n",
        "- Print the loss at every 25th iteration.\n",
        "- Test the model on $x=10$.\n",
        "\n",
        "Note:\n",
        "- If your model does not achieve a loss of less than 0.01, you will still be awarded marks for `Q7 – Q10` as long as you can explain your answers accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdiTwoHS93b3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# 1. Define training data for a the mathematical formula y = x^2 + 3x (4)\n",
        "x = torch.tensor([[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]], dtype=torch.float32)\n",
        "y = torch.tensor([[4], [10], [18], [28], [40], [54], [70], [88], [108], [130]], dtype=torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "# 2. Define NN class (10)\n",
        "\n",
        "class MyNN(nn.Module): # TinyModel is a subclass of nn.Module\n",
        "    \n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(1, hidden_size) # input_size and hidden_size\n",
        "        self.activation = nn.ReLU() # Rectified Linear Unit activation function\n",
        "        self.linear2 = nn.Linear(hidden_size, 1) # hidden layer and output layer size\n",
        "        # output layer is a real value for regression, does not need any activation\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# 3. Create an instance of NN model (2)\n",
        "hidden_size = 300\n",
        "model = MyNN(hidden_size) # now 'model' will be a callable function\n",
        "\n",
        "# 4. Loss and Optimiser (2)\n",
        "learning_rate = 0.01\n",
        "loss_fn = nn.MSELoss() # now 'loss_fn' will be a callable function\n",
        "opt = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# 5. Training loop\n",
        "  # 5.1 Forward pass (2)\n",
        "\n",
        "  # 5.2 Backward pass (3)\n",
        "\n",
        "  # 5.3 Print loss every 25th epoch (2)\n",
        "\n",
        "# Ensure that loss is less than 0.01 at the end if training consistently (3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zatxfhYejeB8"
      },
      "source": [
        "# 6. Prediction (2)\n",
        "# Let's use the model on a new number x, defined as a tensor\n",
        "\n",
        "# Get the model's prediction for this new x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvmHgY1oDLiB"
      },
      "source": [
        "# Make sure the output of your code cells support your answers below:\n",
        "\n",
        "# Q7. Describe how the loss changed over time during training. (2)\n",
        "\n",
        "# Q8. Is the prediction for x=10 close enough to the ideal value of 130? \n",
        "# Why do you think the prediction is or isn't close enough to the ideal value? (2)\n",
        "\n",
        "# Q9. What are the predictions for x=20 and x=100? Based on these predictions, \n",
        "# comment on whether the model has captured the relationship between the training inputs and outputs. (3)\n",
        "\n",
        "# Q10. Apart from tweaking the number of epochs and the number of neuron units in the hidden layer, think\n",
        "# of AT LEAST ONE more thing you would do to try to improve the model. You do NOT have to follow the \n",
        "# requirements nor to implement anything. (3)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}