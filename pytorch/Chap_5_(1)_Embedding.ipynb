{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chap 5-(1) Embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/superbunny38/2021DeepLearning/blob/main/pytorch/Chap_5_(1)_Embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 순환 신경망(RNN, Recurrent Neural Network)\n",
        "\n",
        "usage:\n",
        "- Document Classifiers: 트위터나 리뷰의 감성 파악, 뉴스 기사 분류\n",
        "- Sequence-to-sequence learning: 영어를 프랑스어로 변환하는, 언어 번역과 같은 작업\n",
        "- Time-series forecasting: 전날의 판매 기록에 관한 자세한 사항이 제공될 때 상점의 판매 예측\n",
        "\n",
        "<br><br>\n",
        "**벡터화(vectorization):** text -> (word, character, n-gram) -> number\n",
        "\n",
        "**토큰화(tokenization):** text->token; 텍스트를 토큰(텍스트 단위)으로 나누는 작업\n",
        "\n",
        "**mapping**(one-hot encoding or word embedding): mapping (token->vector)\n",
        "\n",
        "<br>\n",
        "\n",
        "**text->token->number**"
      ],
      "metadata": {
        "id": "7-K8Eq114wmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 토큰화 Tokenization\n",
        "- text -> characters\n",
        "- text -> words\n",
        "- text -> n-gram"
      ],
      "metadata": {
        "id": "uSQn3R-a7q2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "toy_story_review = \"Just perfect. Script, character, animation....this manages to break free of the yoke of 'children's movie' to simply be one of the best movies of the 90's, full-stop.\""
      ],
      "metadata": {
        "id": "zPUL6SPw5EQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text->Char"
      ],
      "metadata": {
        "id": "6A4cWO-PCKlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "character = list(toy_story_review)\n",
        "character"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJgqnvfTB_Lq",
        "outputId": "4c7aa6bf-6d7c-4e31-a66f-116ef9fd4a9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['J',\n",
              " 'u',\n",
              " 's',\n",
              " 't',\n",
              " ' ',\n",
              " 'p',\n",
              " 'e',\n",
              " 'r',\n",
              " 'f',\n",
              " 'e',\n",
              " 'c',\n",
              " 't',\n",
              " '.',\n",
              " ' ',\n",
              " 'S',\n",
              " 'c',\n",
              " 'r',\n",
              " 'i',\n",
              " 'p',\n",
              " 't',\n",
              " ',',\n",
              " ' ',\n",
              " 'c',\n",
              " 'h',\n",
              " 'a',\n",
              " 'r',\n",
              " 'a',\n",
              " 'c',\n",
              " 't',\n",
              " 'e',\n",
              " 'r',\n",
              " ',',\n",
              " ' ',\n",
              " 'a',\n",
              " 'n',\n",
              " 'i',\n",
              " 'm',\n",
              " 'a',\n",
              " 't',\n",
              " 'i',\n",
              " 'o',\n",
              " 'n',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " '.',\n",
              " 't',\n",
              " 'h',\n",
              " 'i',\n",
              " 's',\n",
              " ' ',\n",
              " 'm',\n",
              " 'a',\n",
              " 'n',\n",
              " 'a',\n",
              " 'g',\n",
              " 'e',\n",
              " 's',\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 'b',\n",
              " 'r',\n",
              " 'e',\n",
              " 'a',\n",
              " 'k',\n",
              " ' ',\n",
              " 'f',\n",
              " 'r',\n",
              " 'e',\n",
              " 'e',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 'y',\n",
              " 'o',\n",
              " 'k',\n",
              " 'e',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " \"'\",\n",
              " 'c',\n",
              " 'h',\n",
              " 'i',\n",
              " 'l',\n",
              " 'd',\n",
              " 'r',\n",
              " 'e',\n",
              " 'n',\n",
              " \"'\",\n",
              " 's',\n",
              " ' ',\n",
              " 'm',\n",
              " 'o',\n",
              " 'v',\n",
              " 'i',\n",
              " 'e',\n",
              " \"'\",\n",
              " ' ',\n",
              " 't',\n",
              " 'o',\n",
              " ' ',\n",
              " 's',\n",
              " 'i',\n",
              " 'm',\n",
              " 'p',\n",
              " 'l',\n",
              " 'y',\n",
              " ' ',\n",
              " 'b',\n",
              " 'e',\n",
              " ' ',\n",
              " 'o',\n",
              " 'n',\n",
              " 'e',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " 'b',\n",
              " 'e',\n",
              " 's',\n",
              " 't',\n",
              " ' ',\n",
              " 'm',\n",
              " 'o',\n",
              " 'v',\n",
              " 'i',\n",
              " 'e',\n",
              " 's',\n",
              " ' ',\n",
              " 'o',\n",
              " 'f',\n",
              " ' ',\n",
              " 't',\n",
              " 'h',\n",
              " 'e',\n",
              " ' ',\n",
              " '9',\n",
              " '0',\n",
              " \"'\",\n",
              " 's',\n",
              " ',',\n",
              " ' ',\n",
              " 'f',\n",
              " 'u',\n",
              " 'l',\n",
              " 'l',\n",
              " '-',\n",
              " 's',\n",
              " 't',\n",
              " 'o',\n",
              " 'p',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text->Word"
      ],
      "metadata": {
        "id": "AHycdXiaCVog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(toy_story_review.split())#공백을 구분자로 사용\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyJUg81ACNTp",
        "outputId": "f329c948-d3c4-4c52-d086-4a5b8ee42b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Just',\n",
              " 'perfect.',\n",
              " 'Script,',\n",
              " 'character,',\n",
              " 'animation....this',\n",
              " 'manages',\n",
              " 'to',\n",
              " 'break',\n",
              " 'free',\n",
              " 'of',\n",
              " 'the',\n",
              " 'yoke',\n",
              " 'of',\n",
              " \"'children's\",\n",
              " \"movie'\",\n",
              " 'to',\n",
              " 'simply',\n",
              " 'be',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'best',\n",
              " 'movies',\n",
              " 'of',\n",
              " 'the',\n",
              " \"90's,\",\n",
              " 'full-stop.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-gram\n",
        "n: 함께 사용될 수 있는 단어의 숫자\n",
        "\n",
        "텍스트의 순차적인 특성을 잃어버림 -> 얕은 기계 학습 모델에 자주 사용함.(심층학습에서는 잘 사용되지 않음)"
      ],
      "metadata": {
        "id": "iecQAz2NCx4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "print(list(ngrams(toy_story_review.split(),2)))#bigram(n=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWyU6zvOCpme",
        "outputId": "ca18e81d-8faf-4e1e-c4dd-064e793a2013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Just', 'perfect.'), ('perfect.', 'Script,'), ('Script,', 'character,'), ('character,', 'animation....this'), ('animation....this', 'manages'), ('manages', 'to'), ('to', 'break'), ('break', 'free'), ('free', 'of'), ('of', 'the'), ('the', 'yoke'), ('yoke', 'of'), ('of', \"'children's\"), (\"'children's\", \"movie'\"), (\"movie'\", 'to'), ('to', 'simply'), ('simply', 'be'), ('be', 'one'), ('one', 'of'), ('of', 'the'), ('the', 'best'), ('best', 'movies'), ('movies', 'of'), ('of', 'the'), ('the', \"90's,\"), (\"90's,\", 'full-stop.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(ngrams(toy_story_review.split(),3)))#n=3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2yiYM5dDLei",
        "outputId": "96db9c4b-1dcf-4beb-dec9-19d2a11a8f93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Just', 'perfect.', 'Script,'), ('perfect.', 'Script,', 'character,'), ('Script,', 'character,', 'animation....this'), ('character,', 'animation....this', 'manages'), ('animation....this', 'manages', 'to'), ('manages', 'to', 'break'), ('to', 'break', 'free'), ('break', 'free', 'of'), ('free', 'of', 'the'), ('of', 'the', 'yoke'), ('the', 'yoke', 'of'), ('yoke', 'of', \"'children's\"), ('of', \"'children's\", \"movie'\"), (\"'children's\", \"movie'\", 'to'), (\"movie'\", 'to', 'simply'), ('to', 'simply', 'be'), ('simply', 'be', 'one'), ('be', 'one', 'of'), ('one', 'of', 'the'), ('of', 'the', 'best'), ('the', 'best', 'movies'), ('best', 'movies', 'of'), ('movies', 'of', 'the'), ('of', 'the', \"90's,\"), ('the', \"90's,\", 'full-stop.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 벡터화 Vectorization\n",
        "\n",
        "- one-hot encoding\n",
        "- word embedding"
      ],
      "metadata": {
        "id": "9VIQaB2tDpNh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Hot Encoding"
      ],
      "metadata": {
        "id": "wwY00UiYD6qh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = \"An apple a day keeps doctor away said the doctor\""
      ],
      "metadata": {
        "id": "6z1ENiVzDX1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Dictionary(object):\n",
        "  def __init__(self):\n",
        "    self.word2index = {}#인덱스와 함께 모든 고유한 단어를 저장할 딕셔너리\n",
        "    self.index2word = []#고유한 단어 저장\n",
        "    self.length = 0#고유한 전체 단어의 개수\n",
        "\n",
        "  def add_word(self, word):#단어 추가\n",
        "    if word not in self.index2word:\n",
        "      self.index2word.append(word)\n",
        "      self.word2index[word] = self.length +1#dictionary\n",
        "      self.length +=1#어휘 길이 증가\n",
        "      return self.word2index[word]\n",
        "  \n",
        "  def __len__(self):#어휘 길이 반환\n",
        "    return len(self.index2word)\n",
        "  \n",
        "  def onehot_encoded(self, word):\n",
        "    vec = np.zeros(self.length+1)#0으로 채움\n",
        "    vec[self.word2index[word]] = 1#단어의 인덱스의 벡터 값만 1로 채움\n",
        "    return vec"
      ],
      "metadata": {
        "id": "6F48nkLIFWgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic = Dictionary()\n",
        "for tok in sample_sentence.split():\n",
        "  dic.add_word(tok)"
      ],
      "metadata": {
        "id": "0mBrnSqHF7ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dic.word2index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCuJkwPCGsBy",
        "outputId": "4b9a94ee-0a0b-49a5-e575-c8aae1df8043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'An': 1,\n",
              " 'a': 3,\n",
              " 'apple': 2,\n",
              " 'away': 7,\n",
              " 'day': 4,\n",
              " 'doctor': 6,\n",
              " 'keeps': 5,\n",
              " 'said': 8,\n",
              " 'the': 9}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#index 1부터 채운 one-hot vector\n",
        "for word in dic.index2word:\n",
        "  print(\"word:\",word)\n",
        "  print(dic.onehot_encoded(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9iVVWzbGvZB",
        "outputId": "adc2ad21-31c5-43be-ab0b-b30e7eac766a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: An\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: apple\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: a\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "word: day\n",
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "word: keeps\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "word: doctor\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "word: away\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "word: said\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "word: the\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "toy_story_review 데이터 사용해보기"
      ],
      "metadata": {
        "id": "6iChQ3zLHZNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dic = Dictionary()\n",
        "\n",
        "for tok in toy_story_review.split():\n",
        "  dic.add_word(tok)\n",
        "\n",
        "print(dic.word2index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPBIlJh8G0uD",
        "outputId": "c40c3c7c-2333-422a-920e-397cd65395f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Just': 1, 'perfect.': 2, 'Script,': 3, 'character,': 4, 'animation....this': 5, 'manages': 6, 'to': 7, 'break': 8, 'free': 9, 'of': 10, 'the': 11, 'yoke': 12, \"'children's\": 13, \"movie'\": 14, 'simply': 15, 'be': 16, 'one': 17, 'best': 18, 'movies': 19, \"90's,\": 20, 'full-stop.': 21}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in dic.index2word:\n",
        "  print(\"word:\",word)\n",
        "  print(dic.onehot_encoded(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwN_8MVHHkDS",
        "outputId": "9025964f-51ff-45c5-e946-f9c0ae08675c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word: Just\n",
            "[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: perfect.\n",
            "[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: Script,\n",
            "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: character,\n",
            "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: animation....this\n",
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: manages\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: to\n",
            "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: break\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: free\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: of\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: the\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: yoke\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: 'children's\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: movie'\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "word: simply\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            "word: be\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "word: one\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "word: best\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "word: movies\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            "word: 90's,\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            "word: full-stop.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "one-hot의 문제점: \n",
        "- 데이터가 너무 희소하다(sparse)\n",
        "- 고유한 단어들이 증가함에 따라 벡터의 길이가 빠르게 커진다\n",
        "- 단어들 간의 내부적인 연관성을 표현하지 못한다\n",
        "\n",
        "**-> 심층 학습에서 거의 사용하지 않는다**"
      ],
      "metadata": {
        "id": "aPB4f4I2Huld"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word Embedding\n",
        "- 실수로 채워진 밀집 표현을 제공\n",
        "- 벡터 차원은 하이퍼파라미터\n",
        "- 의미적으로 더 가까운 단어가 더 비슷한 표현을 갖도록 조정\n",
        "- 데이터가 너무 적은 경우 pretrained word embedding 사용\n"
      ],
      "metadata": {
        "id": "b5p0piW6H_tP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 감성 분류기(Sentiment Analysis)를 구축하면서 Word Embedding 학습하기\n",
        "1. IMDb 데이터 다운로드 및 텍스트 토큰화 수행하기\n",
        "2. 어휘 구축하기\n",
        "3. 벡터들의 배치 생성하기\n",
        "4. 임베딩으로 네트워크 모델 생성하기\n",
        "5. 모델 학습하기"
      ],
      "metadata": {
        "id": "PcTrFN9aJfEq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. IMDb 데이터 다운로드 및 텍스트 토큰화 수행하기<1>"
      ],
      "metadata": {
        "id": "vqDvqXBPJ3aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext"
      ],
      "metadata": {
        "id": "r3nhbhq_HoIL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchtext.data로 토큰화(tokenization)하기"
      ],
      "metadata": {
        "id": "1I4BYDI4KKyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import data\n",
        "#Field: 토큰화하는 방법을 정의\n",
        "text = data.Field(lower=True, batch_first = True, fix_length = 20)#실제 텍스트 (모든 텍스트를 소문자(lower)로, 최대길이는 20으로 자름)\n",
        "label = data.Field(sequential = False)#레이블 데이터"
      ],
      "metadata": {
        "id": "2-7pnOyVJ6sb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "torchtext.datasets으로 토큰화(tokenization)하기"
      ],
      "metadata": {
        "id": "Xy_gRI35LsUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext.legacy.datasets"
      ],
      "metadata": {
        "id": "CvUMb3xyOkSU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.legacy import datasets\n",
        "train_data, test_data = datasets.IMDB.splits(text,label)"
      ],
      "metadata": {
        "id": "eyX1yjDuQQrD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc1283d-c465-4b03-faab-00b523fe5f7e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:03<00:00, 26.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train,test = train_data, test_data\n",
        "print(\"train.fields:\")\n",
        "print(train.fields)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOV4IL5TMOzf",
        "outputId": "2f91c2bb-a229-4c6a-e465-2a0921766c0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.fields:\n",
            "{'text': <torchtext.legacy.data.field.Field object at 0x7fe69d5661d0>, 'label': <torchtext.legacy.data.field.Field object at 0x7fe69d566290>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "학습 데이터셋의 변수"
      ],
      "metadata": {
        "id": "MWKxMTFcQ2Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9XGtrCLRUAD",
        "outputId": "ae5cac2d-32b3-4337-b79a-2731985addcb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.legacy.data.example.Example at 0x7fe697fb2f90>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#vars():returns the __dict__ attribute of the given object\n",
        "print(vars(train[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRd7-cc8QrtR",
        "outputId": "54778f60-1d73-4887-8397-24ab8a54e391"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': ['i', 'have', 'to', 'hand', 'it', 'to', 'the', 'creative', 'team', 'behind', 'these', '\"american', 'pie\"', 'movies.', '\"direct', 'to', 'dvd\"', 'typically', 'is', 'synonymous', 'with', 'cheap,', 'incompetent', 'film-making.', 'yet', 'last', 'year', 'i', 'was', 'pleasantly', 'surprised', 'when', 'i', 'found', 'myself', 'thoroughly', 'enjoying', 'the', 'dvd', 'sequel', '\"the', 'naked', 'mile\".', 'the', 'filmmakers', 'took', 'advantage', 'of', 'the', 'opportunity', 'to', 'deliver', 'a', 'raunchy,', 'yet', 'funny', 'little', 'film.', 'this', 'year', 'they', 'offer', 'up', 'the', 'followup,', '\"beta', 'house\".', 'this', 'is', 'the', 'honest', 'truth,', '\"beta', 'house\"', 'makes', 'the', 'first', 'few', '\"american', 'pie\"', 'movies', 'look', 'like', '\"the', 'little', 'mermaid\".<br', '/><br', '/>this', 'is', 'no', 'holds', 'barred,', 'tasteless,', 'laugh-out', 'loud', 'fun.', 'sure,', 'the', 'story', 'is', 'a', 'bit', 'thin,', 'but', \"that's\", 'the', 'beauty', 'of', 'the', 'whole', 'thing.', 'within', 'the', 'first', '10', 'minutes', \"we're\", 'introduced', 'to', 'the', 'all', 'the', 'main', 'characters,', 'the', 'new', 'supporting', 'characters,', 'get', 'a', 'handful', 'of', 'raunchy', 'gags,', 'meet', 'the', 'villains,', 'and', 'establish', 'the', 'general', 'plot-line.', 'with', 'all', 'that', 'out', 'of', 'the', 'way,', 'the', 'movie', 'becomes', 'a', 'no-limits', 'ride.', 'the', 'gags', 'are', 'a', 'plenty,', 'and', 'they', 'did', 'not', 'hold', 'back', 'in', 'this', 'one.', \"i'm\", 'talking', 'male', 'semen,', 'urine,', 'dildos,', 'chicks-with-dicks,', 'sex', 'with', 'sheep,', 'female', 'orgazim', 'sprays,', 'and', 'plenty', 'more.', 'not', 'to', 'mention', 'the', 'fact', 'that', 'not', 'a', 'minute', 'goes', 'by', 'without', 'boobs', 'or', 'a', 'sex', 'scene.<br', '/><br', '/>returning', 'from', '\"the', 'naked', 'mile\"', 'are', 'john', 'white,', 'jake', 'siegel,', 'steve', 'talley,', 'and', 'eugene', 'levy', '(in', 'a', 'similar', 'supporting', 'role', 'as', 'the', 'last', 'few', 'films).', 'the', 'entire', 'cast', 'does', 'fine', 'work.', 'steve', 'talley', '(dwight', 'stifler),', 'in', 'particular,', 'has', 'a', 'great', 'energy', 'and', 'screen', 'presence.', 'i', 'predict', 'good', 'things', 'for', 'him.', 'the', 'film', 'is', 'also', 'loaded', 'with', 'great', 'movie', 'references', 'for', 'those', 'who', 'keep', 'their', 'eyes', 'open.', 'by', 'far', 'the', 'biggest', 'laugh', 'of', 'the', 'film', 'for', 'me', 'was', '\"the', 'deerhunter\"', 'parody.', 'classic.<br', '/><br', '/>the', 'bottom', 'line', 'is,', 'if', \"you're\", 'a', 'fan', 'of', 'the', 'series,', \"you'll\", 'feel', 'right', 'at', 'home', 'with', '\"beta', 'house\".', 'it', 'really', 'pushes', 'the', 'limits', 'of', 'good', 'taste,', 'but', 'in', 'the', 'end', 'is', 'pretty', 'damn', 'funny.'], 'label': 'pos'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vars(train[0])[\"text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zk2ezO8RVPe",
        "outputId": "ac1737ae-41da-41fd-a543-f638329757b9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'have',\n",
              " 'to',\n",
              " 'hand',\n",
              " 'it',\n",
              " 'to',\n",
              " 'the',\n",
              " 'creative',\n",
              " 'team',\n",
              " 'behind',\n",
              " 'these',\n",
              " '\"american',\n",
              " 'pie\"',\n",
              " 'movies.',\n",
              " '\"direct',\n",
              " 'to',\n",
              " 'dvd\"',\n",
              " 'typically',\n",
              " 'is',\n",
              " 'synonymous',\n",
              " 'with',\n",
              " 'cheap,',\n",
              " 'incompetent',\n",
              " 'film-making.',\n",
              " 'yet',\n",
              " 'last',\n",
              " 'year',\n",
              " 'i',\n",
              " 'was',\n",
              " 'pleasantly',\n",
              " 'surprised',\n",
              " 'when',\n",
              " 'i',\n",
              " 'found',\n",
              " 'myself',\n",
              " 'thoroughly',\n",
              " 'enjoying',\n",
              " 'the',\n",
              " 'dvd',\n",
              " 'sequel',\n",
              " '\"the',\n",
              " 'naked',\n",
              " 'mile\".',\n",
              " 'the',\n",
              " 'filmmakers',\n",
              " 'took',\n",
              " 'advantage',\n",
              " 'of',\n",
              " 'the',\n",
              " 'opportunity',\n",
              " 'to',\n",
              " 'deliver',\n",
              " 'a',\n",
              " 'raunchy,',\n",
              " 'yet',\n",
              " 'funny',\n",
              " 'little',\n",
              " 'film.',\n",
              " 'this',\n",
              " 'year',\n",
              " 'they',\n",
              " 'offer',\n",
              " 'up',\n",
              " 'the',\n",
              " 'followup,',\n",
              " '\"beta',\n",
              " 'house\".',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'honest',\n",
              " 'truth,',\n",
              " '\"beta',\n",
              " 'house\"',\n",
              " 'makes',\n",
              " 'the',\n",
              " 'first',\n",
              " 'few',\n",
              " '\"american',\n",
              " 'pie\"',\n",
              " 'movies',\n",
              " 'look',\n",
              " 'like',\n",
              " '\"the',\n",
              " 'little',\n",
              " 'mermaid\".<br',\n",
              " '/><br',\n",
              " '/>this',\n",
              " 'is',\n",
              " 'no',\n",
              " 'holds',\n",
              " 'barred,',\n",
              " 'tasteless,',\n",
              " 'laugh-out',\n",
              " 'loud',\n",
              " 'fun.',\n",
              " 'sure,',\n",
              " 'the',\n",
              " 'story',\n",
              " 'is',\n",
              " 'a',\n",
              " 'bit',\n",
              " 'thin,',\n",
              " 'but',\n",
              " \"that's\",\n",
              " 'the',\n",
              " 'beauty',\n",
              " 'of',\n",
              " 'the',\n",
              " 'whole',\n",
              " 'thing.',\n",
              " 'within',\n",
              " 'the',\n",
              " 'first',\n",
              " '10',\n",
              " 'minutes',\n",
              " \"we're\",\n",
              " 'introduced',\n",
              " 'to',\n",
              " 'the',\n",
              " 'all',\n",
              " 'the',\n",
              " 'main',\n",
              " 'characters,',\n",
              " 'the',\n",
              " 'new',\n",
              " 'supporting',\n",
              " 'characters,',\n",
              " 'get',\n",
              " 'a',\n",
              " 'handful',\n",
              " 'of',\n",
              " 'raunchy',\n",
              " 'gags,',\n",
              " 'meet',\n",
              " 'the',\n",
              " 'villains,',\n",
              " 'and',\n",
              " 'establish',\n",
              " 'the',\n",
              " 'general',\n",
              " 'plot-line.',\n",
              " 'with',\n",
              " 'all',\n",
              " 'that',\n",
              " 'out',\n",
              " 'of',\n",
              " 'the',\n",
              " 'way,',\n",
              " 'the',\n",
              " 'movie',\n",
              " 'becomes',\n",
              " 'a',\n",
              " 'no-limits',\n",
              " 'ride.',\n",
              " 'the',\n",
              " 'gags',\n",
              " 'are',\n",
              " 'a',\n",
              " 'plenty,',\n",
              " 'and',\n",
              " 'they',\n",
              " 'did',\n",
              " 'not',\n",
              " 'hold',\n",
              " 'back',\n",
              " 'in',\n",
              " 'this',\n",
              " 'one.',\n",
              " \"i'm\",\n",
              " 'talking',\n",
              " 'male',\n",
              " 'semen,',\n",
              " 'urine,',\n",
              " 'dildos,',\n",
              " 'chicks-with-dicks,',\n",
              " 'sex',\n",
              " 'with',\n",
              " 'sheep,',\n",
              " 'female',\n",
              " 'orgazim',\n",
              " 'sprays,',\n",
              " 'and',\n",
              " 'plenty',\n",
              " 'more.',\n",
              " 'not',\n",
              " 'to',\n",
              " 'mention',\n",
              " 'the',\n",
              " 'fact',\n",
              " 'that',\n",
              " 'not',\n",
              " 'a',\n",
              " 'minute',\n",
              " 'goes',\n",
              " 'by',\n",
              " 'without',\n",
              " 'boobs',\n",
              " 'or',\n",
              " 'a',\n",
              " 'sex',\n",
              " 'scene.<br',\n",
              " '/><br',\n",
              " '/>returning',\n",
              " 'from',\n",
              " '\"the',\n",
              " 'naked',\n",
              " 'mile\"',\n",
              " 'are',\n",
              " 'john',\n",
              " 'white,',\n",
              " 'jake',\n",
              " 'siegel,',\n",
              " 'steve',\n",
              " 'talley,',\n",
              " 'and',\n",
              " 'eugene',\n",
              " 'levy',\n",
              " '(in',\n",
              " 'a',\n",
              " 'similar',\n",
              " 'supporting',\n",
              " 'role',\n",
              " 'as',\n",
              " 'the',\n",
              " 'last',\n",
              " 'few',\n",
              " 'films).',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'cast',\n",
              " 'does',\n",
              " 'fine',\n",
              " 'work.',\n",
              " 'steve',\n",
              " 'talley',\n",
              " '(dwight',\n",
              " 'stifler),',\n",
              " 'in',\n",
              " 'particular,',\n",
              " 'has',\n",
              " 'a',\n",
              " 'great',\n",
              " 'energy',\n",
              " 'and',\n",
              " 'screen',\n",
              " 'presence.',\n",
              " 'i',\n",
              " 'predict',\n",
              " 'good',\n",
              " 'things',\n",
              " 'for',\n",
              " 'him.',\n",
              " 'the',\n",
              " 'film',\n",
              " 'is',\n",
              " 'also',\n",
              " 'loaded',\n",
              " 'with',\n",
              " 'great',\n",
              " 'movie',\n",
              " 'references',\n",
              " 'for',\n",
              " 'those',\n",
              " 'who',\n",
              " 'keep',\n",
              " 'their',\n",
              " 'eyes',\n",
              " 'open.',\n",
              " 'by',\n",
              " 'far',\n",
              " 'the',\n",
              " 'biggest',\n",
              " 'laugh',\n",
              " 'of',\n",
              " 'the',\n",
              " 'film',\n",
              " 'for',\n",
              " 'me',\n",
              " 'was',\n",
              " '\"the',\n",
              " 'deerhunter\"',\n",
              " 'parody.',\n",
              " 'classic.<br',\n",
              " '/><br',\n",
              " '/>the',\n",
              " 'bottom',\n",
              " 'line',\n",
              " 'is,',\n",
              " 'if',\n",
              " \"you're\",\n",
              " 'a',\n",
              " 'fan',\n",
              " 'of',\n",
              " 'the',\n",
              " 'series,',\n",
              " \"you'll\",\n",
              " 'feel',\n",
              " 'right',\n",
              " 'at',\n",
              " 'home',\n",
              " 'with',\n",
              " '\"beta',\n",
              " 'house\".',\n",
              " 'it',\n",
              " 'really',\n",
              " 'pushes',\n",
              " 'the',\n",
              " 'limits',\n",
              " 'of',\n",
              " 'good',\n",
              " 'taste,',\n",
              " 'but',\n",
              " 'in',\n",
              " 'the',\n",
              " 'end',\n",
              " 'is',\n",
              " 'pretty',\n",
              " 'damn',\n",
              " 'funny.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vars(train[0])[\"label\"]#label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "iA-f550jRdaj",
        "outputId": "ccf1b2ed-7698-4b5b-f72f-cc5669b17196"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'pos'"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. 어휘Vocabulary 구축하기<2>"
      ],
      "metadata": {
        "id": "NVrMeIM4Rn5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#어휘를 구축하는데 필요한 train 객체 전달\n",
        "#사전에 학습된 가중치 이용\n",
        "#6B: trained on Wikipedia 2014 corpus of 6 billion words\n",
        "#차원 크기:300, 단어의 숫자 10000개로 제한, 10번 이상 출현하지 않은 단어는 제거\n",
        "text.build_vocab(train, vectors = torchtext.vocab.GloVe(name=\"6B\", # trained on Wikipedia 2014 corpus\n",
        "                              dim=300), max_size= 10000, min_freq = 10)\n",
        "label.build_vocab(train)"
      ],
      "metadata": {
        "id": "Xc0fyNSRRfkX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99f7653e-2e58-4675-86bb-328ade67c597"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:51<00:00, 7746.06it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#단어별 출현 빈도\n",
        "text.vocab.freqs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y_SWtxISyHA",
        "outputId": "4f86ac75-9a1e-44d1-8e68-3bb50e5202eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'i': 70480,\n",
              "         'have': 27344,\n",
              "         'to': 133967,\n",
              "         'hand': 651,\n",
              "         'it': 65505,\n",
              "         'the': 322198,\n",
              "         'creative': 299,\n",
              "         'team': 579,\n",
              "         'behind': 1131,\n",
              "         'these': 5233,\n",
              "         '\"american': 71,\n",
              "         'pie\"': 10,\n",
              "         'movies.': 922,\n",
              "         '\"direct': 2,\n",
              "         'dvd\"': 3,\n",
              "         'typically': 111,\n",
              "         'is': 104171,\n",
              "         'synonymous': 13,\n",
              "         'with': 42729,\n",
              "         'cheap,': 93,\n",
              "         'incompetent': 75,\n",
              "         'film-making.': 44,\n",
              "         'yet': 2157,\n",
              "         'last': 2699,\n",
              "         'year': 1406,\n",
              "         'was': 47024,\n",
              "         'pleasantly': 124,\n",
              "         'surprised': 695,\n",
              "         'when': 13609,\n",
              "         'found': 2494,\n",
              "         'myself': 855,\n",
              "         'thoroughly': 334,\n",
              "         'enjoying': 158,\n",
              "         'dvd': 1616,\n",
              "         'sequel': 489,\n",
              "         '\"the': 2714,\n",
              "         'naked': 344,\n",
              "         'mile\".': 2,\n",
              "         'filmmakers': 476,\n",
              "         'took': 1091,\n",
              "         'advantage': 126,\n",
              "         'of': 144462,\n",
              "         'opportunity': 316,\n",
              "         'deliver': 279,\n",
              "         'a': 159953,\n",
              "         'raunchy,': 6,\n",
              "         'funny': 2502,\n",
              "         'little': 6122,\n",
              "         'film.': 4490,\n",
              "         'this': 69714,\n",
              "         'they': 20624,\n",
              "         'offer': 278,\n",
              "         'up': 10776,\n",
              "         'followup,': 1,\n",
              "         '\"beta': 3,\n",
              "         'house\".': 10,\n",
              "         'honest': 281,\n",
              "         'truth,': 87,\n",
              "         'house\"': 33,\n",
              "         'makes': 4150,\n",
              "         'first': 7985,\n",
              "         'few': 3950,\n",
              "         'movies': 5255,\n",
              "         'look': 3691,\n",
              "         'like': 18779,\n",
              "         'mermaid\".<br': 1,\n",
              "         '/><br': 50935,\n",
              "         '/>this': 2259,\n",
              "         'no': 11273,\n",
              "         'holds': 285,\n",
              "         'barred,': 3,\n",
              "         'tasteless,': 11,\n",
              "         'laugh-out': 2,\n",
              "         'loud': 260,\n",
              "         'fun.': 335,\n",
              "         'sure,': 336,\n",
              "         'story': 8743,\n",
              "         'bit': 2760,\n",
              "         'thin,': 36,\n",
              "         'but': 39764,\n",
              "         \"that's\": 3193,\n",
              "         'beauty': 454,\n",
              "         'whole': 2896,\n",
              "         'thing.': 346,\n",
              "         'within': 761,\n",
              "         '10': 1351,\n",
              "         'minutes': 2079,\n",
              "         \"we're\": 487,\n",
              "         'introduced': 268,\n",
              "         'all': 19740,\n",
              "         'main': 2224,\n",
              "         'characters,': 820,\n",
              "         'new': 3994,\n",
              "         'supporting': 885,\n",
              "         'get': 8959,\n",
              "         'handful': 121,\n",
              "         'raunchy': 32,\n",
              "         'gags,': 43,\n",
              "         'meet': 586,\n",
              "         'villains,': 39,\n",
              "         'and': 158572,\n",
              "         'establish': 54,\n",
              "         'general': 555,\n",
              "         'plot-line.': 3,\n",
              "         'that': 66292,\n",
              "         'out': 14510,\n",
              "         'way,': 840,\n",
              "         'movie': 30887,\n",
              "         'becomes': 1367,\n",
              "         'no-limits': 1,\n",
              "         'ride.': 59,\n",
              "         'gags': 187,\n",
              "         'are': 28743,\n",
              "         'plenty,': 4,\n",
              "         'did': 5514,\n",
              "         'not': 28597,\n",
              "         'hold': 519,\n",
              "         'back': 4086,\n",
              "         'in': 90527,\n",
              "         'one.': 1171,\n",
              "         \"i'm\": 4174,\n",
              "         'talking': 825,\n",
              "         'male': 558,\n",
              "         'semen,': 1,\n",
              "         'urine,': 2,\n",
              "         'dildos,': 1,\n",
              "         'chicks-with-dicks,': 1,\n",
              "         'sex': 1181,\n",
              "         'sheep,': 6,\n",
              "         'female': 844,\n",
              "         'orgazim': 1,\n",
              "         'sprays,': 1,\n",
              "         'plenty': 594,\n",
              "         'more.': 456,\n",
              "         'mention': 744,\n",
              "         'fact': 2555,\n",
              "         'minute': 535,\n",
              "         'goes': 2274,\n",
              "         'by': 21976,\n",
              "         'without': 3145,\n",
              "         'boobs': 22,\n",
              "         'or': 16769,\n",
              "         'scene.<br': 81,\n",
              "         '/>returning': 3,\n",
              "         'from': 19934,\n",
              "         'mile\"': 4,\n",
              "         'john': 1887,\n",
              "         'white,': 106,\n",
              "         'jake': 104,\n",
              "         'siegel,': 1,\n",
              "         'steve': 392,\n",
              "         'talley,': 1,\n",
              "         'eugene': 64,\n",
              "         'levy': 24,\n",
              "         '(in': 463,\n",
              "         'similar': 778,\n",
              "         'role': 2180,\n",
              "         'as': 45102,\n",
              "         'films).': 16,\n",
              "         'entire': 1456,\n",
              "         'cast': 2883,\n",
              "         'does': 5495,\n",
              "         'fine': 1072,\n",
              "         'work.': 585,\n",
              "         'talley': 1,\n",
              "         '(dwight': 3,\n",
              "         'stifler),': 1,\n",
              "         'particular,': 99,\n",
              "         'has': 16570,\n",
              "         'great': 7714,\n",
              "         'energy': 221,\n",
              "         'screen': 1399,\n",
              "         'presence.': 39,\n",
              "         'predict': 64,\n",
              "         'good': 11926,\n",
              "         'things': 3123,\n",
              "         'for': 42843,\n",
              "         'him.': 1124,\n",
              "         'film': 27777,\n",
              "         'also': 8007,\n",
              "         'loaded': 74,\n",
              "         'references': 202,\n",
              "         'those': 4545,\n",
              "         'who': 19407,\n",
              "         'keep': 1572,\n",
              "         'their': 11317,\n",
              "         'eyes': 798,\n",
              "         'open.': 25,\n",
              "         'far': 2582,\n",
              "         'biggest': 505,\n",
              "         'laugh': 905,\n",
              "         'me': 7722,\n",
              "         'deerhunter\"': 1,\n",
              "         'parody.': 19,\n",
              "         'classic.<br': 31,\n",
              "         '/>the': 7409,\n",
              "         'bottom': 311,\n",
              "         'line': 1240,\n",
              "         'is,': 1369,\n",
              "         'if': 15189,\n",
              "         \"you're\": 1858,\n",
              "         'fan': 1432,\n",
              "         'series,': 431,\n",
              "         \"you'll\": 1236,\n",
              "         'feel': 2700,\n",
              "         'right': 2366,\n",
              "         'at': 22731,\n",
              "         'home': 1301,\n",
              "         'really': 11065,\n",
              "         'pushes': 52,\n",
              "         'limits': 59,\n",
              "         'taste,': 56,\n",
              "         'end': 3547,\n",
              "         'pretty': 3520,\n",
              "         'damn': 314,\n",
              "         'funny.': 661,\n",
              "         'while': 4686,\n",
              "         'original': 2672,\n",
              "         '1932': 20,\n",
              "         'version,': 188,\n",
              "         'preston': 60,\n",
              "         'foster,': 17,\n",
              "         'good,': 988,\n",
              "         \"there's\": 2814,\n",
              "         'remake': 437,\n",
              "         'more': 13170,\n",
              "         'worthy': 279,\n",
              "         'than': 9807,\n",
              "         '1959': 23,\n",
              "         'one,': 819,\n",
              "         'impossible': 409,\n",
              "         'find': 4013,\n",
              "         'anywhere,': 28,\n",
              "         'just': 17309,\n",
              "         'strongly': 210,\n",
              "         'suspect': 240,\n",
              "         'mickey': 91,\n",
              "         'rooney': 52,\n",
              "         'had': 11042,\n",
              "         'something': 4387,\n",
              "         'do': 7904,\n",
              "         'that.': 861,\n",
              "         'never': 6319,\n",
              "         'could': 7594,\n",
              "         'mere': 176,\n",
              "         'performance': 2133,\n",
              "         'ever': 5077,\n",
              "         'been': 9074,\n",
              "         'so': 18099,\n",
              "         'masterfully': 53,\n",
              "         'brilliant,': 116,\n",
              "         'script': 2128,\n",
              "         'thought-provoking,': 12,\n",
              "         'well': 5855,\n",
              "         'an': 21240,\n",
              "         'improvement': 57,\n",
              "         'upon': 772,\n",
              "         'original.': 249,\n",
              "         'many': 6388,\n",
              "         'years': 3335,\n",
              "         'after': 7118,\n",
              "         'my': 11766,\n",
              "         'several': 1393,\n",
              "         'viewings': 49,\n",
              "         'film,': 4042,\n",
              "         '1970,': 14,\n",
              "         'read': 1774,\n",
              "         'article': 37,\n",
              "         'which': 10898,\n",
              "         'recounting': 13,\n",
              "         'visit': 229,\n",
              "         \"he'd\": 179,\n",
              "         'made': 7041,\n",
              "         'death': 1205,\n",
              "         'row,': 27,\n",
              "         'apparently': 720,\n",
              "         'very': 13633,\n",
              "         'drastically': 31,\n",
              "         'eliminated': 22,\n",
              "         'whatever': 550,\n",
              "         'sense': 1690,\n",
              "         'personal': 593,\n",
              "         'identification': 13,\n",
              "         'felt': 1479,\n",
              "         'people': 7676,\n",
              "         'circumstances.': 49,\n",
              "         'about': 16486,\n",
              "         'short': 1358,\n",
              "         'character': 5260,\n",
              "         'here,': 851,\n",
              "         \"didn't\": 4147,\n",
              "         'cover': 410,\n",
              "         'much,': 264,\n",
              "         'other': 8229,\n",
              "         'extent': 107,\n",
              "         'his': 29059,\n",
              "         'extreme': 269,\n",
              "         'disillusionment': 11,\n",
              "         'quality': 972,\n",
              "         'inmates': 45,\n",
              "         'themselves': 778,\n",
              "         'emphasized,': 2,\n",
              "         'even': 12010,\n",
              "         'language': 328,\n",
              "         'would': 12027,\n",
              "         'care': 1174,\n",
              "         'explicitly': 22,\n",
              "         'quote': 119,\n",
              "         'here.': 838,\n",
              "         '.': 1314,\n",
              "         'one': 22480,\n",
              "         'problems': 613,\n",
              "         'capital': 76,\n",
              "         'punishment': 49,\n",
              "         'that,': 1286,\n",
              "         'course,': 980,\n",
              "         'evenly,': 1,\n",
              "         'impartially': 1,\n",
              "         'applied,': 1,\n",
              "         'innocent': 346,\n",
              "         'far-too-carelessly,': 1,\n",
              "         'thus': 324,\n",
              "         'unnecessarily': 42,\n",
              "         'sent': 389,\n",
              "         'particular': 575,\n",
              "         'fate.': 33,\n",
              "         'another': 3686,\n",
              "         'problem': 1160,\n",
              "         'applied': 45,\n",
              "         'swiftly': 11,\n",
              "         'enough,': 362,\n",
              "         'or,': 127,\n",
              "         'matter,': 113,\n",
              "         'publicly': 17,\n",
              "         'enough!': 6,\n",
              "         'bible': 88,\n",
              "         'special': 1812,\n",
              "         'point,': 329,\n",
              "         'such': 4812,\n",
              "         'cases,': 30,\n",
              "         'important': 779,\n",
              "         'purposes': 32,\n",
              "         'such,': 82,\n",
              "         'deterrent,': 1,\n",
              "         'being': 6390,\n",
              "         'ineffectually': 1,\n",
              "         'obscured,': 3,\n",
              "         'minus,': 2,\n",
              "         'only': 11566,\n",
              "         'public': 440,\n",
              "         'viewing,': 61,\n",
              "         'direct': 284,\n",
              "         'participation': 28,\n",
              "         'all!': 54,\n",
              "         'claim': 199,\n",
              "         'prove,': 3,\n",
              "         'statistically,': 2,\n",
              "         'effective': 391,\n",
              "         'deterrent?': 1,\n",
              "         'addition': 255,\n",
              "         'having': 2433,\n",
              "         'reliability': 1,\n",
              "         'data,': 4,\n",
              "         'any': 7507,\n",
              "         'objectively': 10,\n",
              "         'disprovable': 1,\n",
              "         'doubt': 622,\n",
              "         'bars': 38,\n",
              "         'now': 3006,\n",
              "         'due': 853,\n",
              "         'deterrent': 1,\n",
              "         'lacking.': 33,\n",
              "         'however,': 2346,\n",
              "         'robert': 822,\n",
              "         'duvall,': 13,\n",
              "         'apostle,': 1,\n",
              "         'punished': 25,\n",
              "         'all,': 1473,\n",
              "         '\"crime,\"': 1,\n",
              "         'hope': 1292,\n",
              "         'leniency': 1,\n",
              "         'he': 26177,\n",
              "         'be': 25691,\n",
              "         'based': 1260,\n",
              "         'on': 31619,\n",
              "         '\"temporary': 1,\n",
              "         'insanity\"': 1,\n",
              "         'defense,': 12,\n",
              "         'though': 3141,\n",
              "         'serve': 160,\n",
              "         'acceptable': 98,\n",
              "         'excuse': 384,\n",
              "         'kind': 2545,\n",
              "         'case.': 136,\n",
              "         'various': 597,\n",
              "         'questions': 367,\n",
              "         'concerning': 113,\n",
              "         'motives': 74,\n",
              "         'recounted,': 1,\n",
              "         'answers': 129,\n",
              "         'can': 10797,\n",
              "         'try': 1656,\n",
              "         'speculate,': 3,\n",
              "         'decidedly': 58,\n",
              "         'religious': 274,\n",
              "         'nature.': 69,\n",
              "         \"don't\": 7879,\n",
              "         'know': 5336,\n",
              "         'exactly': 947,\n",
              "         'become': 1488,\n",
              "         'professing': 6,\n",
              "         'christian': 306,\n",
              "         'whenever': 263,\n",
              "         'possible,': 75,\n",
              "         'emphasize': 35,\n",
              "         'is;': 28,\n",
              "         'but,': 745,\n",
              "         'anybody': 251,\n",
              "         'should': 4842,\n",
              "         'well-aware,': 1,\n",
              "         'category': 105,\n",
              "         'tends': 86,\n",
              "         'most': 8477,\n",
              "         'vehemently': 6,\n",
              "         'blood,': 132,\n",
              "         'comes': 2434,\n",
              "         'extracting': 8,\n",
              "         'eye': 495,\n",
              "         'eye.': 66,\n",
              "         'bone': 68,\n",
              "         'contention': 15,\n",
              "         'per': 125,\n",
              "         'se,': 12,\n",
              "         'doubt,': 82,\n",
              "         'scripturally': 1,\n",
              "         'speaking,': 39,\n",
              "         'perhaps': 1407,\n",
              "         'most,': 54,\n",
              "         'shall': 119,\n",
              "         'spared': 24,\n",
              "         'same': 3770,\n",
              "         'ultimate': 242,\n",
              "         'fate,': 27,\n",
              "         'hands': 456,\n",
              "         'lord': 236,\n",
              "         'himself,': 222,\n",
              "         'result': 485,\n",
              "         'sacrifice': 84,\n",
              "         'cross.': 10,\n",
              "         'there': 13094,\n",
              "         'problem,': 76,\n",
              "         'me,': 1060,\n",
              "         'spirit': 372,\n",
              "         'attitude': 170,\n",
              "         'christians': 66,\n",
              "         'enthusiasm': 65,\n",
              "         'punishment;': 1,\n",
              "         'for,': 157,\n",
              "         'contrary': 67,\n",
              "         'love': 5326,\n",
              "         'see': 10410,\n",
              "         'everybody': 306,\n",
              "         'saved': 244,\n",
              "         '(ezekiel': 1,\n",
              "         '18:32)': 1,\n",
              "         '(ii': 1,\n",
              "         'peter': 665,\n",
              "         '3:9),': 1,\n",
              "         'seem': 2142,\n",
              "         'go': 4552,\n",
              "         'vindictively': 3,\n",
              "         'way': 5815,\n",
              "         'reasons': 370,\n",
              "         'condemn!': 1,\n",
              "         'what': 14055,\n",
              "         'people,': 496,\n",
              "         'either': 1182,\n",
              "         'side': 889,\n",
              "         'superlatively': 3,\n",
              "         'ever-burning': 1,\n",
              "         'issue,': 14,\n",
              "         'cannot': 1089,\n",
              "         'appear': 571,\n",
              "         'sufficiently': 23,\n",
              "         'appreciate,': 9,\n",
              "         'dynamically': 2,\n",
              "         'elusively': 1,\n",
              "         'soft': 180,\n",
              "         'nature': 519,\n",
              "         'hard.': 71,\n",
              "         'two': 6210,\n",
              "         'sides': 112,\n",
              "         'inherently': 28,\n",
              "         'incompatible': 7,\n",
              "         'render': 31,\n",
              "         'him': 6385,\n",
              "         'mentally': 146,\n",
              "         'deranged,': 5,\n",
              "         'least': 2741,\n",
              "         'strictly': 125,\n",
              "         'human': 1374,\n",
              "         'reckoning.': 2,\n",
              "         'yet,': 292,\n",
              "         'regardless': 83,\n",
              "         'how': 8456,\n",
              "         'harrowingly': 2,\n",
              "         'ungraspable': 2,\n",
              "         'miraculously': 34,\n",
              "         'dynamic': 99,\n",
              "         'blending': 28,\n",
              "         'water': 323,\n",
              "         'oil': 125,\n",
              "         'surely': 374,\n",
              "         'anything': 2475,\n",
              "         'it,': 3006,\n",
              "         'fanatically': 4,\n",
              "         'characteristically': 5,\n",
              "         'equation,': 4,\n",
              "         'falls': 810,\n",
              "         'inadequately': 3,\n",
              "         'unacceptably': 1,\n",
              "         'judicial': 6,\n",
              "         'truth.': 100,\n",
              "         'indeed,': 192,\n",
              "         \"i've\": 3157,\n",
              "         'seen': 5333,\n",
              "         'blood-curdling': 5,\n",
              "         'thirst': 17,\n",
              "         'come': 2927,\n",
              "         'out,': 632,\n",
              "         'self-contradictorily': 1,\n",
              "         'far-too-many': 1,\n",
              "         'occasions,': 18,\n",
              "         'categorically': 1,\n",
              "         'anti-death': 4,\n",
              "         'penalty': 23,\n",
              "         'advocates': 4,\n",
              "         'confronted,': 1,\n",
              "         'rationally': 5,\n",
              "         'well-balanced': 4,\n",
              "         'ways,': 137,\n",
              "         'although': 2043,\n",
              "         'died': 365,\n",
              "         'everybody,': 23,\n",
              "         'thereby': 46,\n",
              "         'going': 3828,\n",
              "         'saved.': 6,\n",
              "         'after-all,': 1,\n",
              "         'order': 815,\n",
              "         'receive': 105,\n",
              "         'absolution,': 3,\n",
              "         'must,': 22,\n",
              "         'repeat': 121,\n",
              "         'term,': 11,\n",
              "         'reach': 228,\n",
              "         'repent': 4,\n",
              "         '(luke': 9,\n",
              "         '13:3-5).': 1,\n",
              "         'make': 7590,\n",
              "         'sense?': 7,\n",
              "         'then,': 540,\n",
              "         \"lord's\": 14,\n",
              "         'command': 80,\n",
              "         'forgive,': 1,\n",
              "         'case': 984,\n",
              "         \"one's\": 246,\n",
              "         'enemies,': 11,\n",
              "         'despise': 39,\n",
              "         'persecute': 2,\n",
              "         'you': 27564,\n",
              "         'cause': 394,\n",
              "         'provocation?': 1,\n",
              "         'far-too-prevailing': 1,\n",
              "         'difficulties': 60,\n",
              "         'sentimentality,': 6,\n",
              "         'popularly': 2,\n",
              "         'misinterpreted,': 3,\n",
              "         'obscuringly': 1,\n",
              "         'over-simplifies': 1,\n",
              "         'real': 4026,\n",
              "         'meaning': 334,\n",
              "         'forgiveness.': 8,\n",
              "         'act': 878,\n",
              "         'forgiveness': 23,\n",
              "         'not,': 380,\n",
              "         'itself,': 215,\n",
              "         'mean': 1136,\n",
              "         'thing': 3514,\n",
              "         'unconditionally': 1,\n",
              "         'excusing': 2,\n",
              "         'forgiven.': 10,\n",
              "         'takes': 2131,\n",
              "         'clearly': 837,\n",
              "         'sober,': 8,\n",
              "         'view': 663,\n",
              "         'perspective': 157,\n",
              "         \"god's\": 80,\n",
              "         'own': 2926,\n",
              "         'attitude,': 25,\n",
              "         'actually': 3911,\n",
              "         'amounts': 88,\n",
              "         'fervent': 9,\n",
              "         'wish': 894,\n",
              "         'forgiven': 41,\n",
              "         'will': 8926,\n",
              "         'ultimately': 427,\n",
              "         'succeed': 98,\n",
              "         'finding': 335,\n",
              "         'seeing': 1876,\n",
              "         'light,': 73,\n",
              "         'granted': 81,\n",
              "         'mercy.': 7,\n",
              "         'opposite': 187,\n",
              "         'of,': 175,\n",
              "         'say,': 612,\n",
              "         'jonah,': 4,\n",
              "         'resented': 1,\n",
              "         'god': 641,\n",
              "         'told': 913,\n",
              "         'preaching': 33,\n",
              "         'nineveh': 1,\n",
              "         'repentance.': 2,\n",
              "         'jonah': 4,\n",
              "         'want': 3567,\n",
              "         'them': 5442,\n",
              "         'repent,': 1,\n",
              "         'desired': 38,\n",
              "         'destroyed.': 23,\n",
              "         'self-righteously,': 1,\n",
              "         'cold-bloodedly': 1,\n",
              "         'unto': 19,\n",
              "         'was,': 417,\n",
              "         'save': 959,\n",
              "         'were': 10528,\n",
              "         'undoubtedly': 95,\n",
              "         'better': 4495,\n",
              "         'most!': 2,\n",
              "         'envy': 24,\n",
              "         'almost': 3019,\n",
              "         'much': 8739,\n",
              "         'me!': 66,\n",
              "         'minus': 55,\n",
              "         'repentance': 3,\n",
              "         'forgiven,': 3,\n",
              "         'may': 3274,\n",
              "         'genuine': 226,\n",
              "         'good.': 1065,\n",
              "         'case,': 284,\n",
              "         'benefit': 92,\n",
              "         'himself!': 14,\n",
              "         'kudos': 95,\n",
              "         'cesar': 21,\n",
              "         'montano': 6,\n",
              "         'reviving': 12,\n",
              "         'cebuano': 4,\n",
              "         'movie!': 317,\n",
              "         'panaghoy': 4,\n",
              "         'sa': 7,\n",
              "         'suba': 3,\n",
              "         '--': 1672,\n",
              "         'drama,': 224,\n",
              "         'action,': 331,\n",
              "         'romance,': 117,\n",
              "         'scene': 4060,\n",
              "         'laugh.<br': 23,\n",
              "         '/>while': 251,\n",
              "         '(a': 551,\n",
              "         'triangle': 45,\n",
              "         'four-cornered-love,': 1,\n",
              "         'japanese': 636,\n",
              "         'occupation,': 4,\n",
              "         'rebellion,': 7,\n",
              "         'american': 1839,\n",
              "         'lord),': 1,\n",
              "         'its': 7963,\n",
              "         'presentation': 97,\n",
              "         'cool,': 111,\n",
              "         'especially': 2166,\n",
              "         'uses': 526,\n",
              "         'bisaya': 1,\n",
              "         'filipino,': 2,\n",
              "         'nipongo': 1,\n",
              "         'english': 749,\n",
              "         'american.<br': 2,\n",
              "         \"year's\": 80,\n",
              "         'best': 5426,\n",
              "         'pinoy': 2,\n",
              "         'movies.<br': 117,\n",
              "         '/>go': 28,\n",
              "         'watch': 5709,\n",
              "         'this!': 90,\n",
              "         'accidentally': 180,\n",
              "         'stumbled': 77,\n",
              "         'over': 5027,\n",
              "         'tv': 2058,\n",
              "         'day.': 353,\n",
              "         'aired': 106,\n",
              "         'middle': 719,\n",
              "         'day': 1536,\n",
              "         'channel': 297,\n",
              "         'famous': 689,\n",
              "         'airing': 25,\n",
              "         'nothing': 3762,\n",
              "         'less': 1655,\n",
              "         'then': 6945,\n",
              "         'good.<br': 125,\n",
              "         '/>october': 1,\n",
              "         'sky': 189,\n",
              "         'tells': 873,\n",
              "         'true': 1790,\n",
              "         'homer': 67,\n",
              "         'hickam,': 3,\n",
              "         'boy': 988,\n",
              "         'inspired': 287,\n",
              "         'sputnick': 2,\n",
              "         'launch': 46,\n",
              "         'rocket': 77,\n",
              "         'scientist.': 12,\n",
              "         'friends': 1218,\n",
              "         'begin': 583,\n",
              "         'build': 283,\n",
              "         'rockets.': 2,\n",
              "         'father': 1315,\n",
              "         'happy': 710,\n",
              "         'sons': 88,\n",
              "         'hobby': 14,\n",
              "         'rather': 2619,\n",
              "         'coal-miner': 2,\n",
              "         'himself': 1527,\n",
              "         'college': 381,\n",
              "         'football-scholarship': 1,\n",
              "         'brother.<br': 11,\n",
              "         'written.': 79,\n",
              "         'too': 6142,\n",
              "         'predictable': 504,\n",
              "         'maybe,': 67,\n",
              "         'ok': 385,\n",
              "         \"doesn't\": 4424,\n",
              "         'focus': 437,\n",
              "         'parts': 906,\n",
              "         'story.': 1077,\n",
              "         \"it's\": 15970,\n",
              "         'part,': 272,\n",
              "         'where': 6056,\n",
              "         'obvious': 907,\n",
              "         'inner': 185,\n",
              "         'action': 2403,\n",
              "         'between': 3260,\n",
              "         'characters': 5263,\n",
              "         'focused': 171,\n",
              "         'on.': 687,\n",
              "         'some': 15280,\n",
              "         'clichés,': 52,\n",
              "         'ok.': 101,\n",
              "         'actual': 783,\n",
              "         \"event's\": 3,\n",
              "         \"can't\": 3442,\n",
              "         'drop': 173,\n",
              "         'clichés.': 39,\n",
              "         'downhill': 66,\n",
              "         'brought': 727,\n",
              "         'manage': 267,\n",
              "         'high.': 51,\n",
              "         'sympathy': 161,\n",
              "         'them.': 1253,\n",
              "         'written': 1165,\n",
              "         'believable.<br': 35,\n",
              "         'looking': 2110,\n",
              "         'movie.': 5337,\n",
              "         'sets': 665,\n",
              "         \"50's\": 83,\n",
              "         'style': 1020,\n",
              "         'thorough': 17,\n",
              "         'pictures': 318,\n",
              "         'composed': 93,\n",
              "         'lit.': 6,\n",
              "         'mood': 325,\n",
              "         'acting': 4747,\n",
              "         'gyllenhaal': 17,\n",
              "         'delivers': 323,\n",
              "         'hickam': 7,\n",
              "         'chris': 342,\n",
              "         'cooper': 108,\n",
              "         'hickam.': 1,\n",
              "         'rest': 1655,\n",
              "         'too.': 662,\n",
              "         'together': 1446,\n",
              "         'strong': 917,\n",
              "         'cast.<br': 41,\n",
              "         '/>all': 415,\n",
              "         'glad': 437,\n",
              "         'caught': 505,\n",
              "         'learned': 238,\n",
              "         'events.': 71,\n",
              "         'known': 870,\n",
              "         'probably': 2724,\n",
              "         'interesting.': 310,\n",
              "         'october': 30,\n",
              "         'interesting': 2364,\n",
              "         'believe': 2310,\n",
              "         'everyone': 1795,\n",
              "         'enjoy.': 115,\n",
              "         'feel-good': 54,\n",
              "         'bad': 6816,\n",
              "         'came': 1635,\n",
              "         'late': 985,\n",
              "         'am': 2667,\n",
              "         'watched': 2115,\n",
              "         'both': 3133,\n",
              "         'together...which': 1,\n",
              "         'compliment': 31,\n",
              "         'makers': 363,\n",
              "         'flick': 633,\n",
              "         'giving': 820,\n",
              "         'pure': 513,\n",
              "         'basic': 463,\n",
              "         'treatment': 169,\n",
              "         'idea': 1674,\n",
              "         'romanticism...': 1,\n",
              "         'marginally': 29,\n",
              "         'separating': 10,\n",
              "         'relationships!': 1,\n",
              "         'lot': 3611,\n",
              "         'already,': 62,\n",
              "         'appropriate': 172,\n",
              "         'highlight': 153,\n",
              "         'portions': 35,\n",
              "         'personally': 304,\n",
              "         'loved.<br': 5,\n",
              "         '/>i': 3735,\n",
              "         'think': 6773,\n",
              "         'point': 2312,\n",
              "         'jesse': 102,\n",
              "         'celine': 24,\n",
              "         'phony': 63,\n",
              "         'phone': 255,\n",
              "         'calls': 235,\n",
              "         'respective': 73,\n",
              "         'shrewd': 15,\n",
              "         'telling': 509,\n",
              "         'each': 2525,\n",
              "         'meant': 593,\n",
              "         'through': 4455,\n",
              "         'journey': 319,\n",
              "         'extending': 8,\n",
              "         '24': 88,\n",
              "         'hrs...': 1,\n",
              "         'curiosity': 77,\n",
              "         'infallible': 5,\n",
              "         'impact': 273,\n",
              "         'smartly': 12,\n",
              "         'dealt': 135,\n",
              "         'with...<br': 3,\n",
              "         '/>on': 192,\n",
              "         'plot': 4776,\n",
              "         'front': 524,\n",
              "         ',': 844,\n",
              "         'making': 2426,\n",
              "         'romantic': 741,\n",
              "         'work': 2917,\n",
              "         'conversation': 143,\n",
              "         'easy': 669,\n",
              "         'job': 1632,\n",
              "         'accomplish..<br': 1,\n",
              "         'flicks': 207,\n",
              "         'flavor': 43,\n",
              "         'designed': 172,\n",
              "         \"writer's\": 51,\n",
              "         \"director's\": 239,\n",
              "         'mind.': 244,\n",
              "         'actors': 3300,\n",
              "         'bring': 842,\n",
              "         '..': 88,\n",
              "         'wrong': 973,\n",
              "         'bearers': 3,\n",
              "         'difficult': 647,\n",
              "         'justify...': 1,\n",
              "         'character,': 634,\n",
              "         'life': 3866,\n",
              "         'actor': 1570,\n",
              "         'gives': 1562,\n",
              "         'beyond': 786,\n",
              "         'instructions': 15,\n",
              "         'story...here': 1,\n",
              "         'job!': 17,\n",
              "         'kudos..!!!and': 1,\n",
              "         'before': 3430,\n",
              "         'sunset': 22,\n",
              "         'feather': 14,\n",
              "         'beautiful!': 17,\n",
              "         'utterly': 444,\n",
              "         'hilarious.': 174,\n",
              "         'clicks': 7,\n",
              "         'immediately': 381,\n",
              "         'frame': 156,\n",
              "         'us': 3199,\n",
              "         'wonderful': 1357,\n",
              "         'ride': 268,\n",
              "         'spoofing': 20,\n",
              "         'gangster': 208,\n",
              "         'films.': 735,\n",
              "         'conflict': 196,\n",
              "         'brother': 691,\n",
              "         'vs.': 177,\n",
              "         'appears': 779,\n",
              "         \"johnny's\": 16,\n",
              "         'do-gooder': 5,\n",
              "         'd.a.': 9,\n",
              "         'crimelord': 2,\n",
              "         'rival,': 18,\n",
              "         'overly': 229,\n",
              "         'accented': 12,\n",
              "         'moroni.': 1,\n",
              "         'johnny': 234,\n",
              "         'says': 911,\n",
              "         '\"that': 50,\n",
              "         'man': 3872,\n",
              "         'arrested': 101,\n",
              "         'butchering': 16,\n",
              "         'language.\"<br': 1,\n",
              "         '/>check': 16,\n",
              "         'video.': 129,\n",
              "         'worth': 2154,\n",
              "         'look.': 100,\n",
              "         'lisa': 132,\n",
              "         'baumer': 4,\n",
              "         '(ida': 5,\n",
              "         'galli)': 2,\n",
              "         'adulteress': 6,\n",
              "         'wife': 1431,\n",
              "         'big': 3083,\n",
              "         'businessman': 65,\n",
              "         'inherits': 17,\n",
              "         '$1million': 1,\n",
              "         'insurance': 60,\n",
              "         'her': 16540,\n",
              "         'husband': 644,\n",
              "         'killed': 854,\n",
              "         'plane': 235,\n",
              "         'crash': 120,\n",
              "         'business': 400,\n",
              "         'trip': 370,\n",
              "         '.initially': 1,\n",
              "         'she': 12234,\n",
              "         'suspected': 51,\n",
              "         'responsible': 251,\n",
              "         'husbands': 49,\n",
              "         'recently': 459,\n",
              "         'changed': 401,\n",
              "         'investigator': 38,\n",
              "         'lynch': 158,\n",
              "         '(george': 38,\n",
              "         'hilton)and': 1,\n",
              "         'interpol': 6,\n",
              "         'agent': 243,\n",
              "         'tail': 50,\n",
              "         'sure.': 89,\n",
              "         'travels': 97,\n",
              "         'athens,': 4,\n",
              "         'greece': 12,\n",
              "         'cash': 159,\n",
              "         'inheritance,': 3,\n",
              "         'insists': 67,\n",
              "         'cash...a': 1,\n",
              "         'dangerous': 235,\n",
              "         'turn': 1182,\n",
              "         \"who's\": 657,\n",
              "         'identity': 177,\n",
              "         'tries': 1248,\n",
              "         'protect': 154,\n",
              "         'against': 1435,\n",
              "         'lover': 212,\n",
              "         'lara': 32,\n",
              "         'florakis': 1,\n",
              "         'nevertheless': 83,\n",
              "         'along': 1481,\n",
              "         'henchman': 32,\n",
              "         'sharif': 12,\n",
              "         'kill': 1059,\n",
              "         'share': 339,\n",
              "         'money': 1639,\n",
              "         'deems': 5,\n",
              "         'entitled': 65,\n",
              "         'to.': 315,\n",
              "         'our': 2428,\n",
              "         'masked': 38,\n",
              "         'killer': 989,\n",
              "         'starts': 1169,\n",
              "         'his/her': 32,\n",
              "         'brutal': 240,\n",
              "         'killings.': 19,\n",
              "         'customary': 19,\n",
              "         'hero': 604,\n",
              "         'greek': 101,\n",
              "         'police': 917,\n",
              "         'warned': 81,\n",
              "         'leave': 975,\n",
              "         'athens': 4,\n",
              "         'inspector': 128,\n",
              "         'stavros(luigi': 2,\n",
              "         ...})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text.vocab.vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXHEHZs7Vvt5",
        "outputId": "4fc6afe0-c15e-48da-c247-68aab9f77608"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.7724, -0.1800,  0.2072,  ...,  0.6736,  0.2263, -0.2919],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dict\n",
        "for idx, key in enumerate(text.vocab.stoi.keys()):#strings to numerical identifiers.\n",
        "  if idx>9:\n",
        "    break\n",
        "  print(key,\":\",text.vocab.stoi[key])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqVt1gnQVwyO",
        "outputId": "56158b55-261b-4fbd-d510-17ed2cf7c196"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk> : 0\n",
            "<pad> : 1\n",
            "the : 2\n",
            "a : 3\n",
            "and : 4\n",
            "of : 5\n",
            "to : 6\n",
            "is : 7\n",
            "in : 8\n",
            "i : 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#list\n",
        "text.vocab.itos[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dA9AhbbqXnXa",
        "outputId": "e6fe3765-864b-41f3-df2d-84ec7bbe4cd1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'i']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label.vocab.itos#0:unknown, 1:neg, 2:pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n46hnV3ibQuD",
        "outputId": "b6eb0538-b316-4d19-aa2c-bd8802374fbe"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', 'neg', 'pos']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. 벡터들의 배치 생성하기<3>\n",
        "BucketIterator사용: 모든 텍스트의 배치를 생성하고 단어들을 단어들의 인덱스 번호로 대체"
      ],
      "metadata": {
        "id": "WMf96rbRXwmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter = data.BucketIterator.splits((train, test), batch_size = 128,\n",
        "                                                   device = None,#use gpu. device = -1: cpu\n",
        "                                                   shuffle = True)"
      ],
      "metadata": {
        "id": "OvfkvGBbXkMX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))"
      ],
      "metadata": {
        "id": "2ajp0myka-QO"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch.text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sFPkXvybDRN",
        "outputId": "ce6383cd-0c6a-4bfb-d0fc-c4885dd40c50"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[   0,    0,    0,  ...,   10, 1309, 1069],\n",
              "        [  12,    7,  314,  ...,    0,   91,   30],\n",
              "        [ 133,    9,  386,  ...,   86, 1004,   12],\n",
              "        ...,\n",
              "        [ 192,   12,   62,  ...,   69,   66,  338],\n",
              "        [ 675, 8031, 4539,  ...,  820,   11, 1295],\n",
              "        [   0,    7,    8,  ...,    7,   30,    5]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(batch.text)#batch size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4c5o6QNbHJ4",
        "outputId": "d0d4806f-f820-482d-e924-999fc2ce8372"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "128"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(batch.text[0])#fixed length = 20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JpX8KVVbpdj",
        "outputId": "b017613c-4bc5-4803-bf78-e3b5cadc60c3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch.label#0:unknown, 1:neg, 2:pos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mp-dYmibEyn",
        "outputId": "7e32c845-f0cd-4c23-914b-139134c49264"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 2,\n",
              "        2, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1,\n",
              "        1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 1, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 2, 2,\n",
              "        2, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 2, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 2,\n",
              "        2, 2, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 2, 1, 1, 1, 2, 2, 1, 2,\n",
              "        1, 2, 1, 1, 2, 2, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. 임베딩으로 네트워크 모델 생성하기<4>\n",
        "단어 임베딩을 생성하고 각 리뷰의 감성(sentiment)을 예측하는 모델 학습"
      ],
      "metadata": {
        "id": "cjvAa_O3bwBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmbeddingNetwork(nn.Module):\n",
        "  def __init__(self, emb_size, hidden_size1, hidden_size2 = 400):#감정분류기 모델\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(emb_size, hidden_size1,padding_idx=0)#어휘의 크기, 차원의 크기\n",
        "    self.fc = nn.Linear(hidden_size2, 3)#label 3개\n",
        "  def forward(self, x):\n",
        "    #print(\"x.size(0):\",x.size(0))#128\n",
        "    embeds = self.embedding(x).view(x.size(0),-1)#다른 배치들과 섞이는 것 막음\n",
        "    out = self.fc(embeds)\n",
        "    return F.log_softmax(out, dim = -1)#출력 크기: 배치 사이즈 x fixed_length x 차원"
      ],
      "metadata": {
        "id": "rYjPlllKbGDt"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. 모델 학습하기<5>"
      ],
      "metadata": {
        "id": "L0ViFpTbi9K9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(optimizer, epoch, model, data_loader, phase = 'training', volatile = False):\n",
        "  if phase == \"training\":\n",
        "    model.train()\n",
        "  if phase == \"validation\":\n",
        "    model.eval()\n",
        "\n",
        "  volatile = True\n",
        "  running_loss = 0.0\n",
        "  running_correct = 0\n",
        "\n",
        "  for batch_idx, batch in enumerate(data_loader):\n",
        "    #print(\"batch: {}/128\".format(batch_idx+1))\n",
        "    text, target = batch.text, batch.label\n",
        "    #print(\"len\",len(batch.text))\n",
        "    try:\n",
        "      text, target = text.cuda(), target.cuda()\n",
        "    except:#no cuda\n",
        "      data, target = Variable(data, volatile), Variable(target)\n",
        "\n",
        "    if phase == \"training\":\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "    output = model(text)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    running_loss += F.nll_loss(output, target, size_average=False).data#[0]\n",
        "    predictions = output.data.max(dim = 1, keepdim = True)[1]\n",
        "    running_correct += predictions.eq(target.data.view_as(predictions)).cpu().sum()\n",
        "\n",
        "    if phase == \"training\":\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    loss = running_loss/len(data_loader.dataset)\n",
        "    accuracy = 100.*running_correct/len(data_loader.dataset)\n",
        "    if batch_idx == 195:\n",
        "      print(f\"batch_idx: {batch_idx} | {phase} loss is {loss:{5}.{2}} and {phase} accuracy is {running_correct}/{len(data_loader.dataset)}{accuracy:{10}.{4}}\".format(loss, accuracy))\n",
        "  return loss, accuracy"
      ],
      "metadata": {
        "id": "ZM8YyIJGdKGm"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EmbeddingNetwork(20*128*10,20)#emb_size, hidden_size1\n",
        "try:\n",
        "  model.cuda()\n",
        "except:\n",
        "  print(\"GPU not available\")"
      ],
      "metadata": {
        "id": "bYkn7Q-os3tZ"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "optimizer = optim.Adam(model.parameters(),lr=0.01)"
      ],
      "metadata": {
        "id": "d6UBdQf_tNih"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses , train_accuracy = [],[] \n",
        "validation_losses , validation_accuracy = [],[]\n",
        "\n",
        "train_iter.repeat = False#배치 생성 멈춤\n",
        "test_iter.repeat = False#배치 생성 멈춤\n",
        "for epoch in range(1,10):\n",
        "    print(\"================epoch{}/19===============\".format(epoch))\n",
        "    epoch_loss, epoch_accuracy = fit(optimizer, epoch,model,train_iter,phase='training')\n",
        "    validation_epoch_loss, validation_epoch_accuracy = fit(optimizer, epoch,model,test_iter,phase='validation')\n",
        "    train_losses.append(epoch_loss) \n",
        "    train_accuracy.append(epoch_accuracy) \n",
        "    validation_losses.append(validation_epoch_loss) \n",
        "    validation_accuracy.append(validation_epoch_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CxdROKDrfNl",
        "outputId": "a32e0292-5e1a-4c0c-c42b-5d9c137ef7be"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================epoch1/19===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_idx: 195 | training loss is  0.77 and training accuracy is 13659/25000     54.64\n",
            "batch_idx: 195 | validation loss is  0.71 and validation accuracy is 15022/25000     60.09\n",
            "================epoch2/19===============\n",
            "batch_idx: 195 | training loss is  0.62 and training accuracy is 16892/25000     67.57\n",
            "batch_idx: 195 | validation loss is  0.68 and validation accuracy is 16203/25000     64.81\n",
            "================epoch3/19===============\n",
            "batch_idx: 195 | training loss is  0.51 and training accuracy is 18860/25000     75.44\n",
            "batch_idx: 195 | validation loss is  0.72 and validation accuracy is 16413/25000     65.65\n",
            "================epoch4/19===============\n",
            "batch_idx: 195 | training loss is   0.4 and training accuracy is 20566/25000     82.26\n",
            "batch_idx: 195 | validation loss is  0.84 and validation accuracy is 16429/25000     65.72\n",
            "================epoch5/19===============\n",
            "batch_idx: 195 | training loss is  0.28 and training accuracy is 22077/25000     88.31\n",
            "batch_idx: 195 | validation loss is   1.0 and validation accuracy is 16151/25000      64.6\n",
            "================epoch6/19===============\n",
            "batch_idx: 195 | training loss is  0.17 and training accuracy is 23460/25000     93.84\n",
            "batch_idx: 195 | validation loss is   1.3 and validation accuracy is 16080/25000     64.32\n",
            "================epoch7/19===============\n",
            "batch_idx: 195 | training loss is 0.088 and training accuracy is 24305/25000     97.22\n",
            "batch_idx: 195 | validation loss is   1.5 and validation accuracy is 16026/25000      64.1\n",
            "================epoch8/19===============\n",
            "batch_idx: 195 | training loss is 0.039 and training accuracy is 24784/25000     99.14\n",
            "batch_idx: 195 | validation loss is   1.8 and validation accuracy is 15893/25000     63.57\n",
            "================epoch9/19===============\n",
            "batch_idx: 195 | training loss is 0.017 and training accuracy is 24954/25000     99.82\n",
            "batch_idx: 195 | validation loss is   1.9 and validation accuracy is 15910/25000     63.64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 사전 학습된 단어 임베딩 사용하기\n",
        "1. 임베딩 다운로드하기\n",
        "2. 모델에 임베딩 불러오기\n",
        "3. 임베딩 레이어 가중치 고정하기"
      ],
      "metadata": {
        "id": "nGyy-dP7FK9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 임베딩 다운로드하기<1>"
      ],
      "metadata": {
        "id": "MP6qy9jpFyGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.vocab import GloVe"
      ],
      "metadata": {
        "id": "PRp3pmdBtX82"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.build_vocab(train, vectors = GloVe(name = '6B',dim = 300), max_size = 10000, min_freq = 10)\n",
        "label.build_vocab(train,)"
      ],
      "metadata": {
        "id": "rmP6D5uxGWB0"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text.vocab.vectors#임베딩에 접근"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lW1pO2nGhJ0",
        "outputId": "740e5394-1164-4b52-d879-e510e98c98e9"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.0466,  0.2132, -0.0074,  ...,  0.0091, -0.2099,  0.0539],\n",
              "        ...,\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
              "        [ 0.7724, -0.1800,  0.2072,  ...,  0.6736,  0.2263, -0.2919],\n",
              "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.vocab.vectors)#vocab_size x dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUJs6F7iGteY",
        "outputId": "9229b57b-fd64-4f64-97fc-e122a25e3536"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10002"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.vocab.vectors[0])#fixed length = 300"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TYLMhkwGwQY",
        "outputId": "3cb8c190-9917-45d2-8740-a86578c21650"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 모델에 임베딩 불러오기<2>\n",
        "임베딩을 임베딩 레이어의 가중치에 저장"
      ],
      "metadata": {
        "id": "G7R2EhCGG6TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.embedding.weight.data = text.vocab.vectors#임베딩 레이어의 가중치에 접근하여 임베딩 가중치를 지정"
      ],
      "metadata": {
        "id": "fUap2QsIGx89"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingNetwork(nn.Module):\n",
        "  def __init__(self, embedding_size, hidden_size1, hidden_size2 = 400):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(embedding_size, hidden_size1)\n",
        "    self.fc1 = nn.Linear(hidden_size2, 3)\n",
        "  \n",
        "  def forward(self,x):\n",
        "    embeds = self.embedding(x).view(x.size(0),-1)#batch size\n",
        "    out = self.fc1(embeds)\n",
        "    return F.log_softmax(out,dim = -1)"
      ],
      "metadata": {
        "id": "rFV6ZxTmHTj1"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text.vocab.stoi)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXuCW19UIdLh",
        "outputId": "df2076a4-1149-4337-e95b-8797b12f1aee"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10002"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EmbeddingNetwork(len(text.vocab.stoi),600,12000)#책:model = EmbeddingNetwork(len(text.vocab.stoi),300,12000)"
      ],
      "metadata": {
        "id": "hyPYKLqEIT_V"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  model.cuda()\n",
        "except:\n",
        "  print(\"GPU not available\")"
      ],
      "metadata": {
        "id": "9EZpKvUFI2N2"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 임베딩 레이어 가중치 고정하기<3>\n",
        "1. requires_grad = False: 가중치를 위한 기울기가 필요없음을 지시\n",
        "2. optimizer에 임베딩 레이어의 parameter을 전달하지 않아야 함(아니면 1때문에 에러남)"
      ],
      "metadata": {
        "id": "BB_mUUuDIlfH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.embedding.weight.requires_grad = False\n",
        "optimizer = optim.SGD([param for param in model.parameters() if param.requires_grad == True], lr = 0.001)"
      ],
      "metadata": {
        "id": "-WiN3_qfIgw4"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "훈련"
      ],
      "metadata": {
        "id": "AQ9XoLK6JISA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses , train_accuracy = [],[] \n",
        "validation_losses , validation_accuracy = [],[]\n",
        "\n",
        "train_iter.repeat = False#배치 생성 멈춤\n",
        "test_iter.repeat = False#배치 생성 멈춤\n",
        "for epoch in range(1,19):\n",
        "    print(\"================epoch{}/19===============\".format(epoch))\n",
        "    epoch_loss, epoch_accuracy = fit(optimizer, epoch,model,train_iter,phase='training')\n",
        "    validation_epoch_loss, validation_epoch_accuracy = fit(optimizer, epoch,model,test_iter,phase='validation')\n",
        "    train_losses.append(epoch_loss) \n",
        "    train_accuracy.append(epoch_accuracy) \n",
        "    validation_losses.append(validation_epoch_loss) \n",
        "    validation_accuracy.append(validation_epoch_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT7_Nja4JBjm",
        "outputId": "fa7c4802-3f96-4e5c-9f5f-30fe7b100bc3"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================epoch1/19===============\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_idx: 195 | training loss is  0.54 and training accuracy is 18344/25000     73.38\n",
            "batch_idx: 195 | validation loss is  0.69 and validation accuracy is 14772/25000     59.09\n",
            "================epoch2/19===============\n",
            "batch_idx: 195 | training loss is  0.54 and training accuracy is 18578/25000     74.31\n",
            "batch_idx: 195 | validation loss is  0.69 and validation accuracy is 14799/25000      59.2\n",
            "================epoch3/19===============\n",
            "batch_idx: 195 | training loss is  0.53 and training accuracy is 18700/25000      74.8\n",
            "batch_idx: 195 | validation loss is  0.69 and validation accuracy is 14769/25000     59.08\n",
            "================epoch4/19===============\n",
            "batch_idx: 195 | training loss is  0.52 and training accuracy is 18862/25000     75.45\n",
            "batch_idx: 195 | validation loss is  0.69 and validation accuracy is 14771/25000     59.08\n",
            "================epoch5/19===============\n",
            "batch_idx: 195 | training loss is  0.52 and training accuracy is 18934/25000     75.74\n",
            "batch_idx: 195 | validation loss is  0.69 and validation accuracy is 14788/25000     59.15\n",
            "================epoch6/19===============\n",
            "batch_idx: 195 | training loss is  0.51 and training accuracy is 19062/25000     76.25\n",
            "batch_idx: 195 | validation loss is   0.7 and validation accuracy is 14816/25000     59.26\n",
            "================epoch7/19===============\n",
            "batch_idx: 195 | training loss is   0.5 and training accuracy is 19132/25000     76.53\n",
            "batch_idx: 195 | validation loss is   0.7 and validation accuracy is 14806/25000     59.22\n",
            "================epoch8/19===============\n",
            "batch_idx: 195 | training loss is   0.5 and training accuracy is 19296/25000     77.18\n",
            "batch_idx: 195 | validation loss is   0.7 and validation accuracy is 14788/25000     59.15\n",
            "================epoch9/19===============\n",
            "batch_idx: 195 | training loss is  0.49 and training accuracy is 19337/25000     77.35\n",
            "batch_idx: 195 | validation loss is   0.7 and validation accuracy is 14792/25000     59.17\n",
            "================epoch10/19===============\n",
            "batch_idx: 195 | training loss is  0.49 and training accuracy is 19451/25000      77.8\n",
            "batch_idx: 195 | validation loss is  0.71 and validation accuracy is 14801/25000      59.2\n",
            "================epoch11/19===============\n",
            "batch_idx: 195 | training loss is  0.49 and training accuracy is 19479/25000     77.92\n",
            "batch_idx: 195 | validation loss is  0.71 and validation accuracy is 14794/25000     59.18\n",
            "================epoch12/19===============\n",
            "batch_idx: 195 | training loss is  0.48 and training accuracy is 19557/25000     78.23\n",
            "batch_idx: 195 | validation loss is  0.71 and validation accuracy is 14771/25000     59.08\n",
            "================epoch13/19===============\n",
            "batch_idx: 195 | training loss is  0.48 and training accuracy is 19623/25000     78.49\n",
            "batch_idx: 195 | validation loss is  0.71 and validation accuracy is 14798/25000     59.19\n",
            "================epoch14/19===============\n",
            "batch_idx: 195 | training loss is  0.47 and training accuracy is 19668/25000     78.67\n",
            "batch_idx: 195 | validation loss is  0.71 and validation accuracy is 14819/25000     59.28\n",
            "================epoch15/19===============\n",
            "batch_idx: 195 | training loss is  0.47 and training accuracy is 19777/25000     79.11\n",
            "batch_idx: 195 | validation loss is  0.72 and validation accuracy is 14765/25000     59.06\n",
            "================epoch16/19===============\n",
            "batch_idx: 195 | training loss is  0.47 and training accuracy is 19782/25000     79.13\n",
            "batch_idx: 195 | validation loss is  0.72 and validation accuracy is 14756/25000     59.02\n",
            "================epoch17/19===============\n",
            "batch_idx: 195 | training loss is  0.46 and training accuracy is 19861/25000     79.44\n",
            "batch_idx: 195 | validation loss is  0.72 and validation accuracy is 14765/25000     59.06\n",
            "================epoch18/19===============\n",
            "batch_idx: 195 | training loss is  0.46 and training accuracy is 19877/25000     79.51\n",
            "batch_idx: 195 | validation loss is  0.72 and validation accuracy is 14745/25000     58.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "지금까지의 accuracy가 낮은 이유:\n",
        "**텍스트의 순차적인 특성에 대한 이점을 활용하지 못하기 때문**"
      ],
      "metadata": {
        "id": "_4CYLJ5PJvB2"
      }
    }
  ]
}